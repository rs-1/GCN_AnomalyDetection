
The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu037.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu037.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
slurmstepd: error: *** JOB 20143791 ON spartan-gpgpu037 CANCELLED AT 2020-09-20T16:41:24 ***

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
slurmstepd: error: *** JOB 20143797 ON spartan-gpgpu041 CANCELLED AT 2020-09-20T16:41:24 ***

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
slurmstepd: error: *** JOB 20143798 ON spartan-gpgpu014 CANCELLED AT 2020-09-20T16:41:24 ***

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu042.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu042.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
slurmstepd: error: *** JOB 20143800 ON spartan-gpgpu042 CANCELLED AT 2020-09-20T16:41:24 ***

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu037.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu037.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 64.35671
Epoch: 0020 train_loss= 64.20724
Epoch: 0030 train_loss= 64.20712
Epoch: 0040 train_loss= 64.20712
Epoch: 0050 train_loss= 64.20712
Epoch: 0060 train_loss= 64.20712
Epoch: 0070 train_loss= 64.20712
Epoch: 0080 train_loss= 64.20712
Epoch: 0090 train_loss= 64.20712
Epoch: 0100 train_loss= 64.20712
0.5
Epoch: 0110 train_loss= 64.20712
Epoch: 0120 train_loss= 64.20712
Epoch: 0130 train_loss= 64.20712
Epoch: 0140 train_loss= 64.20712
Epoch: 0150 train_loss= 64.20712
Epoch: 0160 train_loss= 64.20712
Epoch: 0170 train_loss= 64.20712
Epoch: 0180 train_loss= 64.20712
Epoch: 0190 train_loss= 64.20712
Epoch: 0200 train_loss= 64.20712
0.5
Epoch: 0210 train_loss= 64.20712
Epoch: 0220 train_loss= 64.20712
Epoch: 0230 train_loss= 64.20712
Epoch: 0240 train_loss= 64.20712
Epoch: 0250 train_loss= 64.20712
Epoch: 0260 train_loss= 64.20712
Epoch: 0270 train_loss= 64.20712
Epoch: 0280 train_loss= 64.20712
Epoch: 0290 train_loss= 64.20712
Epoch: 0300 train_loss= 64.20712
0.5


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 37.60842
Epoch: 0020 train_loss= 37.60842
Epoch: 0030 train_loss= 37.60842
Epoch: 0040 train_loss= 37.60842
Epoch: 0050 train_loss= 37.60842
Epoch: 0060 train_loss= 37.60842
Epoch: 0070 train_loss= 37.60842
Epoch: 0080 train_loss= 37.60842
Epoch: 0090 train_loss= 37.60842
Epoch: 0100 train_loss= 37.60842
0.43621531346351494
Epoch: 0110 train_loss= 37.60842
Epoch: 0120 train_loss= 37.60842
Epoch: 0130 train_loss= 37.60842
Epoch: 0140 train_loss= 37.60842
Epoch: 0150 train_loss= 37.60842
Epoch: 0160 train_loss= 37.60842
Epoch: 0170 train_loss= 37.60842
Epoch: 0180 train_loss= 37.60842
Epoch: 0190 train_loss= 37.60842
Epoch: 0200 train_loss= 37.60842
0.43621531346351494
Epoch: 0210 train_loss= 37.60842
Epoch: 0220 train_loss= 37.60842
Epoch: 0230 train_loss= 37.60842
Epoch: 0240 train_loss= 37.60842
Epoch: 0250 train_loss= 37.60842
Epoch: 0260 train_loss= 37.60842
Epoch: 0270 train_loss= 37.60842
Epoch: 0280 train_loss= 37.60842
Epoch: 0290 train_loss= 37.60842
Epoch: 0300 train_loss= 37.60842
0.43621531346351494


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 36.04263
Epoch: 0020 train_loss= 36.04263
Epoch: 0030 train_loss= 36.04263
Epoch: 0040 train_loss= 36.04263
Epoch: 0050 train_loss= 36.04263
Epoch: 0060 train_loss= 36.04263
Epoch: 0070 train_loss= 36.04263
Epoch: 0080 train_loss= 36.04263
Epoch: 0090 train_loss= 36.04263
Epoch: 0100 train_loss= 36.04263
0.7315436241610738
Epoch: 0110 train_loss= 36.04263
Epoch: 0120 train_loss= 36.04263
Epoch: 0130 train_loss= 36.04263
Epoch: 0140 train_loss= 36.04263
Epoch: 0150 train_loss= 36.04263
Epoch: 0160 train_loss= 36.04263
Epoch: 0170 train_loss= 36.04263
Epoch: 0180 train_loss= 36.04263
Epoch: 0190 train_loss= 36.04263
Epoch: 0200 train_loss= 36.04263
0.7315436241610738
Epoch: 0210 train_loss= 36.04263
Epoch: 0220 train_loss= 36.04263
Epoch: 0230 train_loss= 36.04263
Epoch: 0240 train_loss= 36.04263
Epoch: 0250 train_loss= 36.04263
Epoch: 0260 train_loss= 36.04263
Epoch: 0270 train_loss= 36.04263
Epoch: 0280 train_loss= 36.04263
Epoch: 0290 train_loss= 36.04263
Epoch: 0300 train_loss= 36.04263
0.7315436241610738


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 10.64576
Epoch: 0020 train_loss= 9.73728
Epoch: 0030 train_loss= 5.56770
Epoch: 0040 train_loss= 5.56763
Epoch: 0050 train_loss= 5.56762
Epoch: 0060 train_loss= 5.56762
Epoch: 0070 train_loss= 5.56761
Epoch: 0080 train_loss= 5.56760
Epoch: 0090 train_loss= 5.56760
Epoch: 0100 train_loss= 5.56760
0.5
Epoch: 0110 train_loss= 5.56759
Epoch: 0120 train_loss= 5.56759
Epoch: 0130 train_loss= 5.56760
Epoch: 0140 train_loss= 5.56759
Epoch: 0150 train_loss= 5.56759
Epoch: 0160 train_loss= 5.56759
Epoch: 0170 train_loss= 5.56759
Epoch: 0180 train_loss= 5.56759
Epoch: 0190 train_loss= 5.56759
Epoch: 0200 train_loss= 5.56759
0.5042372881355932
Epoch: 0210 train_loss= 5.56759
Epoch: 0220 train_loss= 5.56759
Epoch: 0230 train_loss= 5.56759
Epoch: 0240 train_loss= 5.56759
Epoch: 0250 train_loss= 5.56759
Epoch: 0260 train_loss= 5.56759
Epoch: 0270 train_loss= 5.56759
Epoch: 0280 train_loss= 5.56758
Epoch: 0290 train_loss= 5.56758
Epoch: 0300 train_loss= 5.56758
0.5042372881355932


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 114.91814
Epoch: 0020 train_loss= 114.87611
Epoch: 0030 train_loss= 114.80966
Epoch: 0040 train_loss= 58.16700
Epoch: 0050 train_loss= 58.16698
Epoch: 0060 train_loss= 58.16698
Epoch: 0070 train_loss= 58.16698
Epoch: 0080 train_loss= 58.16698
Epoch: 0090 train_loss= 58.16698
Epoch: 0100 train_loss= 58.16698
0.5
Epoch: 0110 train_loss= 58.16698
Epoch: 0120 train_loss= 58.16698
Epoch: 0130 train_loss= 58.16698
Epoch: 0140 train_loss= 58.16698
Epoch: 0150 train_loss= 58.16698
Epoch: 0160 train_loss= 58.16698
Epoch: 0170 train_loss= 58.16698
Epoch: 0180 train_loss= 58.16698
Epoch: 0190 train_loss= 58.16698
Epoch: 0200 train_loss= 58.16698
0.5
Epoch: 0210 train_loss= 58.16698
Epoch: 0220 train_loss= 58.16698
Epoch: 0230 train_loss= 58.16698
Epoch: 0240 train_loss= 58.16698
Epoch: 0250 train_loss= 58.16698
Epoch: 0260 train_loss= 58.16698
Epoch: 0270 train_loss= 58.16698
Epoch: 0280 train_loss= 58.16698
Epoch: 0290 train_loss= 58.16698
Epoch: 0300 train_loss= 58.16698
0.5


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 43.51711
Epoch: 0020 train_loss= 43.51700
Epoch: 0030 train_loss= 43.51700
Epoch: 0040 train_loss= 43.51700
Epoch: 0050 train_loss= 43.51700
Epoch: 0060 train_loss= 43.51700
Epoch: 0070 train_loss= 43.51700
Epoch: 0080 train_loss= 43.51700
Epoch: 0090 train_loss= 43.51700
Epoch: 0100 train_loss= 43.51700
0.7303370786516854
Epoch: 0110 train_loss= 43.51700
Epoch: 0120 train_loss= 43.51700
Epoch: 0130 train_loss= 43.51700
Epoch: 0140 train_loss= 43.51700
Epoch: 0150 train_loss= 43.51700
Epoch: 0160 train_loss= 43.51700
Epoch: 0170 train_loss= 43.51700
Epoch: 0180 train_loss= 43.51700
Epoch: 0190 train_loss= 43.51700
Epoch: 0200 train_loss= 43.51700
0.7303370786516854
Epoch: 0210 train_loss= 43.51700
Epoch: 0220 train_loss= 43.51700
Epoch: 0230 train_loss= 43.51700
Epoch: 0240 train_loss= 43.51700
Epoch: 0250 train_loss= 43.51700
Epoch: 0260 train_loss= 43.51700
Epoch: 0270 train_loss= 43.51700
Epoch: 0280 train_loss= 43.51700
Epoch: 0290 train_loss= 43.51700
Epoch: 0300 train_loss= 43.51700
0.7303370786516854


---------------------------------------- Flickr2 ---------Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
-------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 43.51700
Epoch: 0020 train_loss= 43.51700
Epoch: 0030 train_loss= 43.51700
Epoch: 0040 train_loss= 43.51700
Epoch: 0050 train_loss= 43.51700
Epoch: 0060 train_loss= 43.51700
Epoch: 0070 train_loss= 43.51700
Epoch: 0080 train_loss= 43.51700
Epoch: 0090 train_loss= 43.51700
Epoch: 0100 train_loss= 43.51700
0.7315436241610738
Epoch: 0110 train_loss= 43.51700
Epoch: 0120 train_loss= 43.51700
Epoch: 0130 train_loss= 43.51700
Epoch: 0140 train_loss= 43.51700
Epoch: 0150 train_loss= 43.51700
Epoch: 0160 train_loss= 43.51700
Epoch: 0170 train_loss= 43.51700
Epoch: 0180 train_loss= 43.51700
Epoch: 0190 train_loss= 43.51700
Epoch: 0200 train_loss= 43.51700
0.7315436241610738
Epoch: 0210 train_loss= 43.51700
Epoch: 0220 train_loss= 43.51700
Epoch: 0230 train_loss= 43.51700
Epoch: 0240 train_loss= 43.51700
Epoch: 0250 train_loss= 43.51700
Epoch: 0260 train_loss= 43.51700
Epoch: 0270 train_loss= 43.51700
Epoch: 0280 train_loss= 43.51700
Epoch: 0290 train_loss= 43.51700
Epoch: 0300 train_loss= 43.51700
0.7315436241610738


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 43.51637
Epoch: 0020 train_loss= nan
Epoch: 0030 train_loss= nan
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 84, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 255, in _binary_roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 505, in roc_curve
    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 301, in _binary_clf_curve
    assert_all_finite(y_score)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 65, in assert_all_finite
    _assert_all_finite(X.data if sp.issparse(X) else X)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 57.99968
Epoch: 0020 train_loss= 57.98159
Epoch: 0030 train_loss= 57.98149
Epoch: 0040 train_loss= 57.98145
Epoch: 0050 train_loss= 57.97979
Epoch: 0060 train_loss= 57.90099
Epoch: 0070 train_loss= 57.85071
Epoch: 0080 train_loss= 57.84726
Epoch: 0090 train_loss= 57.84400
Epoch: 0100 train_loss= 57.84249
0.8328073939865016
Epoch: 0110 train_loss= 57.84215
Epoch: 0120 train_loss= 57.84195
Epoch: 0130 train_loss= 57.84183
Epoch: 0140 train_loss= 57.84173
Epoch: 0150 train_loss= 57.84164
Epoch: 0160 train_loss= 57.84157
Epoch: 0170 train_loss= 57.84150
Epoch: 0180 train_loss= 57.84144
Epoch: 0190 train_loss= 57.84138
Epoch: 0200 train_loss= 57.84133
0.8626297760338398
Epoch: 0210 train_loss= 57.84128
Epoch: 0220 train_loss= 57.84124
Epoch: 0230 train_loss= 57.84119
Epoch: 0240 train_loss= 57.84116
Epoch: 0250 train_loss= 57.84113
Epoch: 0260 train_loss= 57.84109
Epoch: 0270 train_loss= 57.84107
Epoch: 0280 train_loss= 57.84104
Epoch: 0290 train_loss= 57.84102
Epoch: 0300 train_loss= 57.84100
0.8637045849672227


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 18507.42969
Epoch: 0020 train_loss= 18505.74414
Epoch: 0030 train_loss= 18505.89844
Epoch: 0040 train_loss= 18505.80469
Epoch: 0050 train_loss= 18505.58594
Epoch: 0060 train_loss= 18505.58984
Epoch: 0070 train_loss= 18505.60352
Epoch: 0080 train_loss= 18505.82812
Epoch: 0090 train_loss= 18505.58203
Epoch: 0100 train_loss= 18505.97656
0.624743062692703
Epoch: 0110 train_loss= 18506.99414
Epoch: 0120 train_loss= 18505.56641
Epoch: 0130 train_loss= 18505.77344
Epoch: 0140 train_loss= 18506.28906
Epoch: 0150 train_loss= 18505.62695
Epoch: 0160 train_loss= 18506.00000
Epoch: 0170 train_loss= 18505.68164
Epoch: 0180 train_loss= 18506.91602
Epoch: 0190 train_loss= 18505.61328
Epoch: 0200 train_loss= 18505.69922
0.6248972250770812
Epoch: 0210 train_loss= 18505.91602
Epoch: 0220 train_loss= 18505.61133
Epoch: 0230 train_loss= 18505.57617
Epoch: 0240 train_loss= 18506.16016
Epoch: 0250 train_loss= 18506.07812
Epoch: 0260 train_loss= 18505.87695
Epoch: 0270 train_loss= 18506.13867
Epoch: 0280 train_loss= 18505.96680
Epoch: 0290 train_loss= 18506.16797
Epoch: 0300 train_loss= 18506.29492
0.6244861253854059


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 32.55802
Epoch: 0020 train_loss= 32.55577
Epoch: 0030 train_loss= 32.55569
Epoch: 0040 train_loss= 32.55569
Epoch: 0050 train_loss= 32.55565
Epoch: 0060 train_loss= 32.55564
Epoch: 0070 train_loss= 32.55564
Epoch: 0080 train_loss= 32.55564
Epoch: 0090 train_loss= 32.55564
Epoch: 0100 train_loss= 32.55564
0.85185879183669
Epoch: 0110 train_loss= 32.55564
Epoch: 0120 train_loss= 32.55563
Epoch: 0130 train_loss= 32.55563
Epoch: 0140 train_loss= 32.55563
Epoch: 0150 train_loss= 32.55563
Epoch: 0160 train_loss= 32.55563
Epoch: 0170 train_loss= 32.55563
Epoch: 0180 train_loss= 32.55563
Epoch: 0190 train_loss= 32.55562
Epoch: 0200 train_loss= 32.55561
0.8519297014806756
Epoch: 0210 train_loss= 32.55561
Epoch: 0220 train_loss= 32.55561
Epoch: 0230 train_loss= 32.55560
Epoch: 0240 train_loss= 32.55560
Epoch: 0250 train_loss= 32.55560
Epoch: 0260 train_loss= 32.55559
Epoch: 0270 train_loss= 32.55559
Epoch: 0280 train_loss= 32.55559
Epoch: 0290 train_loss= 32.55558
Epoch: 0300 train_loss= 32.55558
0.8519194247206776


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 1148.47961
Epoch: 0020 train_loss= 1146.32996
Epoch: 0030 train_loss= 1145.44556
Epoch: 0040 train_loss= 1142.97534
Epoch: 0050 train_loss= 1135.93640
Epoch: 0060 train_loss= 1120.54688
Epoch: 0070 train_loss= 1128.32788
Epoch: 0080 train_loss= 1108.11035
Epoch: 0090 train_loss= 1103.58679
Epoch: 0100 train_loss= 1101.04480
0.5621468926553672
Epoch: 0110 train_loss= 1093.45337
Epoch: 0120 train_loss= 1093.31470
Epoch: 0130 train_loss= 1086.88037
Epoch: 0140 train_loss= 1094.45447
Epoch: 0150 train_loss= 1082.73669
Epoch: 0160 train_loss= 1079.39954
Epoch: 0170 train_loss= 1070.65784
Epoch: 0180 train_loss= 1072.95251
Epoch: 0190 train_loss= 1073.57214
Epoch: 0200 train_loss= 1071.16003
0.5451977401129944
Epoch: 0210 train_loss= 1066.70728
Epoch: 0220 train_loss= 1061.83899
Epoch: 0230 train_loss= 1060.61951
Epoch: 0240 train_loss= 1064.29712
Epoch: 0250 train_loss= 1065.86511
Epoch: 0260 train_loss= 1069.64001
Epoch: 0270 train_loss= 1062.66443
Epoch: 0280 train_loss= 1066.84888
Epoch: 0290 train_loss= 1056.76428
Epoch: 0300 train_loss= 1055.57507
0.5451977401129944


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 114795192.00000
Epoch: 0020 train_loss= 114795192.00000
Epoch: 0030 train_loss= 114795144.00000
Epoch: 0040 train_loss= 114795144.00000
Epoch: 0050 train_loss= 114795144.00000
Epoch: 0060 train_loss= 114795144.00000
Epoch: 0070 train_loss= 114795144.00000
Epoch: 0080 train_loss= 114795144.00000
Epoch: 0090 train_loss= 114795144.00000
Epoch: 0100 train_loss= 114795144.00000
0.7310762862211708
Epoch: 0110 train_loss= 114795144.00000
Epoch: 0120 train_loss= 114795144.00000
Epoch: 0130 train_loss= 114795144.00000
Epoch: 0140 train_loss= 114795144.00000
Epoch: 0150 train_loss= 114795144.00000
Epoch: 0160 train_loss= 114795144.00000
Epoch: 0170 train_loss= 114795144.00000
Epoch: 0180 train_loss= 114795144.00000
Epoch: 0190 train_loss= 114795144.00000
Epoch: 0200 train_loss= 114795144.00000
0.7310762862211709
Epoch: 0210 train_loss= 114795144.00000
Epoch: 0220 train_loss= 114795144.00000
Epoch: 0230 train_loss= 114795144.00000
Epoch: 0240 train_loss= 114795144.00000
Epoch: 0250 train_loss= 114795144.00000
Epoch: 0260 train_loss= 114795144.00000
Epoch: 0270 train_loss= 114795144.00000
Epoch: 0280 train_loss= 114795144.00000
Epoch: 0290 train_loss= 114795144.00000
Epoch: 0300 train_loss= 114795144.00000
0.7310762862211708


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 39.28342
Epoch: 0020 train_loss= nan
Epoch: 0030 train_loss= nan
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
 train_loss= 39.33481
Epoch: 0020 train_loss= 39.33486
Epoch: 0030 train_loss= 39.33486
Epoch: 0040 train_loss= 39.33486
Epoch: 0050 train_loss= 39.33486
Epoch: 0060 train_loss= 39.33486
Epoch: 0070 train_loss= 39.33486
Epoch: 0080 train_loss= 39.33486
Epoch: 0090 train_loss= 39.33486
Epoch: 0100 train_loss= 39.33486
0.8953915867599688
Epoch: 0110 train_loss= 39.33486
Epoch: 0120 train_loss= 39.33486
Epoch: 0130 train_loss= 39.33486
Epoch: 0140 train_loss= 39.33486
Epoch: 0150 train_loss= 39.33486
Epoch: 0160 train_loss= 39.33486
Epoch: 0170 train_loss= 39.33486
Epoch: 0180 train_loss= 39.33486
Epoch: 0190 train_loss= 39.33486
Epoch: 0200 train_loss= 39.33486
0.8953915867599688
Epoch: 0210 train_loss= 39.33486
Epoch: 0220 train_loss= 39.33486
Epoch: 0230 train_loss= 39.33486
Epoch: 0240 train_loss= 39.33486
Epoch: 0250 train_loss= 39.33486
Epoch: 0260 train_loss= 39.33486
Epoch: 0270 train_loss= 39.33486
Epoch: 0280 train_loss= 39.33486
Epoch: 0290 train_loss= 39.33486
Epoch: 0300 train_loss= 39.33486
0.8953915867599688


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 39.87406
Epoch: 0020 train_loss= 39.86919
Epoch: 0030 train_loss= 39.86795
Epoch: 0040 train_loss= 39.86791
Epoch: 0050 train_loss= 39.86791
Epoch: 0060 train_loss= 39.86790
Epoch: 0070 train_loss= 39.86790
Epoch: 0080 train_loss= 39.86790
Epoch: 0090 train_loss= 39.86790
Epoch: 0100 train_loss= 39.86790

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 84, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 255, in _binary_roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 505, in roc_curve
    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 301, in _binary_clf_curve
    assert_all_finite(y_score)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 65, in assert_all_finite
    _assert_all_finite(X.data if sp.issparse(X) else X)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 51.77916
Epoch: 0020 train_loss= 51.54190
Epoch: 0030 train_loss= 51.50719
Epoch: 0040 train_loss= 51.50924
Epoch: 0050 train_loss= 51.50570
Epoch: 0060 train_loss= 51.50586
Epoch: 0070 train_loss= 51.50525
Epoch: 0080 train_loss= 51.50508
Epoch: 0090 train_loss= 51.50487
Epoch: 0100 train_loss= 51.50474
0.8519259506593173
Epoch: 0110 train_loss= 51.50447
Epoch: 0120 train_loss= 51.50417
Epoch: 0130 train_loss= 51.50389
Epoch: 0140 train_loss= 51.50364
Epoch: 0150 train_loss= 51.50344
Epoch: 0160 train_loss= 51.50326
Epoch: 0170 train_loss= 51.47538
Epoch: 0180 train_loss= 51.47459
Epoch: 0190 train_loss= 51.47437
Epoch: 0200 train_loss= 51.47372
0.8686095685100825
Epoch: 0210 train_loss= 51.47329
Epoch: 0220 train_loss= 51.47300
Epoch: 0230 train_loss= 51.47275
Epoch: 0240 train_loss= 51.47254
Epoch: 0250 train_loss= 51.47235
Epoch: 0260 train_loss= 51.47218
Epoch: 0270 train_loss= 51.47203
Epoch: 0280 train_loss= 51.47190
Epoch: 0290 train_loss= 51.47177
Epoch: 0300 train_loss= 51.47165
0.8777598920738354


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 38825.48438
Epoch: 0020 train_loss= 38825.48438
Epoch: 0030 train_loss= 38825.48438
Epoch: 0040 train_loss= 38825.48438
Epoch: 0050 train_loss= 38825.48438
Epoch: 0060 train_loss= 38825.48438
Epoch: 0070 train_loss= 38825.48438
Epoch: 0080 train_loss= 38825.48438
Epoch: 0090 train_loss= 38825.48438
Epoch: 0100 train_loss= 38825.48438
0.436292394655704
Epoch: 0110 train_loss= 38825.48438
Epoch: 0120 train_loss= 38825.48438
Epoch: 0130 train_loss= 38825.48438
Epoch: 0140 train_loss= 38825.48438
Epoch: 0150 train_loss= 38825.48438
Epoch: 0160 train_loss= 38825.48438
Epoch: 0170 train_loss= 38825.48438
Epoch: 0180 train_loss= 38825.48438
Epoch: 0190 train_loss= 38825.48438
Epoch: 0200 train_loss= 38825.48438
0.436292394655704
Epoch: 0210 train_loss= 38825.48438
Epoch: 0220 train_loss= 38825.48438
Epoch: 0230 train_loss= 38825.48438
Epoch: 0240 train_loss= 38825.48438
Epoch: 0250 train_loss= 38825.48438
Epoch: 0260 train_loss= 38825.48438
Epoch: 0270 train_loss= 38825.48438
Epoch: 0280 train_loss= 38825.48438
Epoch: 0290 train_loss= 38825.48438
Epoch: 0300 train_loss= 38825.48438
0.436292394655704


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 29.06957
Epoch: 0020 train_loss= 29.06872
Epoch: 0030 train_loss= 29.06864
Epoch: 0040 train_loss= 29.06862
Epoch: 0050 train_loss= 29.06858
Epoch: 0060 train_loss= 29.06855
Epoch: 0070 train_loss= 29.06853
Epoch: 0080 train_loss= 29.06852
Epoch: 0090 train_loss= 29.06851
Epoch: 0100 train_loss= 29.06849
0.8136480853711007
Epoch: 0110 train_loss= 29.06862
Epoch: 0120 train_loss= 29.06854
Epoch: 0130 train_loss= 29.06849
Epoch: 0140 train_loss= 29.06847
Epoch: 0150 train_loss= 29.06845
Epoch: 0160 train_loss= 29.06845
Epoch: 0170 train_loss= 29.06843
Epoch: 0180 train_loss= 29.06842
Epoch: 0190 train_loss= 29.06841
Epoch: 0200 train_loss= 29.06840
0.8135586775591188
Epoch: 0210 train_loss= 29.06839
Epoch: 0220 train_loss= 29.06841
Epoch: 0230 train_loss= 29.06837
Epoch: 0240 train_loss= 29.06836
Epoch: 0250 train_loss= 29.06834
Epoch: 0260 train_loss= 29.06834
Epoch: 0270 train_loss= 29.06833
Epoch: 0280 train_loss= 29.06831
Epoch: 0290 train_loss= 29.06832
Epoch: 0300 train_loss= 29.06839
0.8135535391791199


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 2281.58472
Epoch: 0020 train_loss= 2269.17065
Epoch: 0030 train_loss= 2245.39038
Epoch: 0040 train_loss= 2208.37915
Epoch: 0050 train_loss= 2179.27490
Epoch: 0060 train_loss= 2165.50537
Epoch: 0070 train_loss= 2181.31396
Epoch: 0080 train_loss= 2149.88184
Epoch: 0090 train_loss= 2138.40454
Epoch: 0100 train_loss= 2136.86328
0.5550847457627119
Epoch: 0110 train_loss= 2140.48901
Epoch: 0120 train_loss= 2115.18018
Epoch: 0130 train_loss= 2115.77222
Epoch: 0140 train_loss= 2136.99414
Epoch: 0150 train_loss= 2110.98804
Epoch: 0160 train_loss= 2127.76074
Epoch: 0170 train_loss= 2131.01538
Epoch: 0180 train_loss= 2105.25830
Epoch: 0190 train_loss= 2133.54858
Epoch: 0200 train_loss= 2127.34058
0.5155367231638418
Epoch: 0210 train_loss= 2120.09082
Epoch: 0220 train_loss= 2107.27539
Epoch: 0230 train_loss= 2102.72632
Epoch: 0240 train_loss= 2121.14429
Epoch: 0250 train_loss= 2086.44116
Epoch: 0260 train_loss= 2095.31860
Epoch: 0270 train_loss= 2084.51270
Epoch: 0280 train_loss= 2111.28687
Epoch: 0290 train_loss= 2114.24780
Epoch: 0300 train_loss= 2082.61963
0.5014124293785311


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 229590272.00000
Epoch: 0020 train_loss= 229590272.00000
Epoch: 0030 train_loss= 226965968.00000
Epoch: 0040 train_loss= 226859776.00000
Epoch: 0050 train_loss= 226870912.00000
Epoch: 0060 train_loss= 226843744.00000
Epoch: 0070 train_loss= 226841440.00000
Epoch: 0080 train_loss= 226841472.00000
Epoch: 0090 train_loss= 226841568.00000
Epoch: 0100 train_loss= 226842496.00000
0.6895623891188646
Epoch: 0110 train_loss= 226842016.00000
Epoch: 0120 train_loss= 226841264.00000
Epoch: 0130 train_loss= 226841296.00000
Epoch: 0140 train_loss= 226841680.00000
Epoch: 0150 train_loss= 226841312.00000
Epoch: 0160 train_loss= 226841120.00000
Epoch: 0170 train_loss= 226841808.00000
Epoch: 0180 train_loss= 226841472.00000
Epoch: 0190 train_loss= 226841184.00000
Epoch: 0200 train_loss= 226840496.00000
0.6901094027202839
Epoch: 0210 train_loss= 226840064.00000
Epoch: 0220 train_loss= 226839776.00000
Epoch: 0230 train_loss= 226841440.00000
Epoch: 0240 train_loss= 226840960.00000
Epoch: 0250 train_loss= 226840240.00000
Epoch: 0260 train_loss= 226840352.00000
Epoch: 0270 train_loss= 226840544.00000
Epoch: 0280 train_loss= 226840224.00000
Epoch: 0290 train_loss= 226839728.00000
Epoch: 0300 train_loss= 226839088.00000
0.6903903015966883


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 35.04988
Epoch: 0020 train_loss= 35.04985
Epoch: 0030 train_loss= 35.04980
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
ETraceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
poch: 0010 train_loss= 35.15297
Epoch: 0020 train_loss= 35.11538
Epoch: 0030 train_loss= 35.06696
Epoch: 0040 train_loss= 35.05345
Epoch: 0050 train_loss= 35.05314
Epoch: 0060 train_loss= 35.05238
Epoch: 0070 train_loss= 35.05231
Epoch: 0080 train_loss= 35.05229
Epoch: 0090 train_loss= 35.05225
Epoch: 0100 train_loss= 35.05225
0.5573429422236282
Epoch: 0110 train_loss= 35.05224
Epoch: 0120 train_loss= 35.05224
Epoch: 0130 train_loss= 35.05224
Epoch: 0140 train_loss= 35.05224
Epoch: 0150 train_loss= 35.05224
Epoch: 0160 train_loss= 35.05224
Epoch: 0170 train_loss= 35.05224
Epoch: 0180 train_loss= 35.05224
Epoch: 0190 train_loss= 35.05224
Epoch: 0200 train_loss= 35.05224
0.5572676177635163
Epoch: 0210 train_loss= 35.05224
Epoch: 0220 train_loss= 35.05224
Epoch: 0230 train_loss= 35.05224
Epoch: 0240 train_loss= 35.05224
Epoch: 0250 train_loss= 35.05224
Epoch: 0260 train_loss= 35.05224
Epoch: 0270 train_loss= 35.05224
Epoch: 0280 train_loss= 35.05224
Epoch: 0290 train_loss= 35.05224
Epoch: 0300 train_loss= 35.05224
0.5572679316154335


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 36.22149
Epoch: 0020 train_loss= nan
Epoch: 0030 train_loss= nan
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu042.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu042.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 45.55530
Epoch: 0020 train_loss= 45.51803
Epoch: 0030 train_loss= 45.42953
Epoch: 0040 train_loss= 45.25663
Epoch: 0050 train_loss= 45.19277
Epoch: 0060 train_loss= 45.18435
Epoch: 0070 train_loss= 45.17982
Epoch: 0080 train_loss= 45.17905
Epoch: 0090 train_loss= 45.17857
Epoch: 0100 train_loss= 45.17819
0.8273110839480462
Epoch: 0110 train_loss= 45.17784
Epoch: 0120 train_loss= 45.17753
Epoch: 0130 train_loss= 45.17723
Epoch: 0140 train_loss= 45.17693
Epoch: 0150 train_loss= 45.17662
Epoch: 0160 train_loss= 45.17630
Epoch: 0170 train_loss= 45.17597
Epoch: 0180 train_loss= 45.17569
Epoch: 0190 train_loss= 45.17543
Epoch: 0200 train_loss= 45.17514
0.8409975422712916
Epoch: 0210 train_loss= 45.17479
Epoch: 0220 train_loss= 45.17453
Epoch: 0230 train_loss= 45.17427
Epoch: 0240 train_loss= 45.17408
Epoch: 0250 train_loss= 45.17390
Epoch: 0260 train_loss= 45.17374
Epoch: 0270 train_loss= 45.17359
Epoch: 0280 train_loss= 45.17345
Epoch: 0290 train_loss= 45.17332
Epoch: 0300 train_loss= 45.17320
0.844252706470054


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 55464.68750
Epoch: 0020 train_loss= 55445.48438
Epoch: 0030 train_loss= 55446.75000
Epoch: 0040 train_loss= 55444.48828
Epoch: 0050 train_loss= 55441.57812
Epoch: 0060 train_loss= 55443.08984
Epoch: 0070 train_loss= 55441.48438
Epoch: 0080 train_loss= 55442.07812
Epoch: 0090 train_loss= 55441.53906
Epoch: 0100 train_loss= 55441.59375
0.6257836587872558
Epoch: 0110 train_loss= 55441.58203
Epoch: 0120 train_loss= 55441.53125
Epoch: 0130 train_loss= 55441.81641
Epoch: 0140 train_loss= 55441.55469
Epoch: 0150 train_loss= 55441.85156
Epoch: 0160 train_loss= 55441.53906
Epoch: 0170 train_loss= 55441.80469
Epoch: 0180 train_loss= 55441.48438
Epoch: 0190 train_loss= 55441.69922
Epoch: 0200 train_loss= 55441.53125
0.6253854059609456
Epoch: 0210 train_loss= 55441.56641
Epoch: 0220 train_loss= 55442.09766
Epoch: 0230 train_loss= 55443.21094
Epoch: 0240 train_loss= 55442.10547
Epoch: 0250 train_loss= 55441.63281
Epoch: 0260 train_loss= 55441.49219
Epoch: 0270 train_loss= 55441.97656
Epoch: 0280 train_loss= 55441.50781
Epoch: 0290 train_loss= 55441.82812
Epoch: 0300 train_loss= 55442.00391
0.6267343268242549


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 25.58218
Epoch: 0020 train_loss= 25.58204
Epoch: 0030 train_loss= 25.58171
Epoch: 0040 train_loss= 25.58171
Epoch: 0050 train_loss= 25.58172
Epoch: 0060 train_loss= 25.58234
Epoch: 0070 train_loss= 25.58234
Epoch: 0080 train_loss= 25.58234
Epoch: 0090 train_loss= 25.58234
Epoch: 0100 train_loss= 25.58234
0.7994990422059682
Epoch: 0110 train_loss= 25.58234
Epoch: 0120 train_loss= 25.58234
Epoch: 0130 train_loss= 25.58234
Epoch: 0140 train_loss= 25.58234
Epoch: 0150 train_loss= 25.58234
Epoch: 0160 train_loss= 25.58234
Epoch: 0170 train_loss= 25.58234
Epoch: 0180 train_loss= 25.58234
Epoch: 0190 train_loss= 25.58234
Epoch: 0200 train_loss= 25.58234
0.7994990422059682
Epoch: 0210 train_loss= 25.58234
Epoch: 0220 train_loss= 25.58234
Epoch: 0230 train_loss= 25.58234
Epoch: 0240 train_loss= 25.58234
Epoch: 0250 train_loss= 25.58234
Epoch: 0260 train_loss= 25.58234
Epoch: 0270 train_loss= 25.58234
Epoch: 0280 train_loss= 25.58234
Epoch: 0290 train_loss= 25.58234
Epoch: 0300 train_loss= 25.58234
0.7994990422059682


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 3425.03394
Epoch: 0020 train_loss= 3405.44043
Epoch: 0030 train_loss= 3380.56616
Epoch: 0040 train_loss= 3346.62549
Epoch: 0050 train_loss= 3325.40308
Epoch: 0060 train_loss= 3296.99805
Epoch: 0070 train_loss= 3273.46875
Epoch: 0080 train_loss= 3257.52173
Epoch: 0090 train_loss= 3312.11108
Epoch: 0100 train_loss= 3335.22632
0.5042372881355932
Epoch: 0110 train_loss= 3269.96338
Epoch: 0120 train_loss= 3359.09424
Epoch: 0130 train_loss= 3304.33057
Epoch: 0140 train_loss= 3281.78296
Epoch: 0150 train_loss= 3258.18726
Epoch: 0160 train_loss= 3230.30908
Epoch: 0170 train_loss= 3216.72192
Epoch: 0180 train_loss= 3199.91162
Epoch: 0190 train_loss= 3194.10132
Epoch: 0200 train_loss= 3212.54810
0.5084745762711864
Epoch: 0210 train_loss= 3212.46313
Epoch: 0220 train_loss= 3174.47876
Epoch: 0230 train_loss= 3173.11304
Epoch: 0240 train_loss= 3155.78418
Epoch: 0250 train_loss= 3190.53442
Epoch: 0260 train_loss= 3147.26465
Epoch: 0270 train_loss= 3216.95776
Epoch: 0280 train_loss= 3166.70483
Epoch: 0290 train_loss= 3203.17383
Epoch: 0300 train_loss= 3172.83374
0.5141242937853108


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 340318912.00000
Epoch: 0020 train_loss= 340265952.00000
Epoch: 0030 train_loss= 340262176.00000
Epoch: 0040 train_loss= 340262016.00000
Epoch: 0050 train_loss= 340262944.00000
Epoch: 0060 train_loss= 340261952.00000
Epoch: 0070 train_loss= 340262336.00000
Epoch: 0080 train_loss= 340262816.00000
Epoch: 0090 train_loss= 340261952.00000
Epoch: 0100 train_loss= 340262048.00000
0.6900946185688942
Epoch: 0110 train_loss= 340261760.00000
Epoch: 0120 train_loss= 340261664.00000
Epoch: 0130 train_loss= 340262048.00000
Epoch: 0140 train_loss= 340261632.00000
Epoch: 0150 train_loss= 340261408.00000
Epoch: 0160 train_loss= 340261632.00000
Epoch: 0170 train_loss= 340261280.00000
Epoch: 0180 train_loss= 340260992.00000
Epoch: 0190 train_loss= 340260832.00000
Epoch: 0200 train_loss= 340259872.00000
0.6902424600827912
Epoch: 0210 train_loss= 340262016.00000
Epoch: 0220 train_loss= 340260640.00000
Epoch: 0230 train_loss= 340263104.00000
Epoch: 0240 train_loss= 340262592.00000
Epoch: 0250 train_loss= 340260704.00000
Epoch: 0260 train_loss= 340264928.00000
Epoch: 0270 train_loss= 340260672.00000
Epoch: 0280 train_loss= 340260384.00000
Epoch: 0290 train_loss= 340260768.00000
Epoch: 0300 train_loss= 340260288.00000
0.6903903015966883


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 30.81630
Epoch: 0020 train_loss= 30.81623
Epoch: 0030 train_loss= 30.81619
Epoch: 0040 train_loss= 30.81618
Epoch: 0050 train_loss= 30.81619
Epoch: 0060 train_loss= 30.81618
Epoch: 0070 train_loss= 30.81618
Epoch: 0080 train_loss= 30.81618
Epoch: 0090 train_loss= 30.81620
Epoch: 0100 train_loss= 30.81617
0.7656305844902848
Epoch: 0110 train_loss= 30.81617
Epoch: 0120 train_loss= 30.81617
Epoch: 0130 train_loss= 30.81617
Epoch: 0140 train_loss= 30.81616
Epoch: 0150 train_loss= 30.81616
Epoch: 0160 train_loss= 30.81616
Epoch: 0170 train_loss= 30.81616
Epoch: 0180 train_loss= 30.81615
Epoch: Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 84, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 255, in _binary_roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 505, in roc_curve
    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 301, in _binary_clf_curve
    assert_all_finite(y_score)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 65, in assert_all_finite
    _assert_all_finite(X.data if sp.issparse(X) else X)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
0190 train_loss= 30.81631
Epoch: 0200 train_loss= 30.81631
0.7654780402477268
Epoch: 0210 train_loss= 30.81631
Epoch: 0220 train_loss= 30.81632
Epoch: 0230 train_loss= 30.81633
Epoch: 0240 train_loss= nan
Epoch: 0250 train_loss= nan
Epoch: 0260 train_loss= nan
Epoch: 0270 train_loss= nan
Epoch: 0280 train_loss= nan
Epoch: 0290 train_loss= nan
Epoch: 0300 train_loss= nan
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 30.97058
Epoch: 0020 train_loss= 30.96654
Epoch: 0030 train_loss= 30.93235
Epoch: 0040 train_loss= 30.83022
Epoch: 0050 train_loss= 30.79684
Epoch: 0060 train_loss= 30.79286
Epoch: 0070 train_loss= 30.79251
Epoch: 0080 train_loss= 30.79206
Epoch: 0090 train_loss= 30.79205
Epoch: 0100 train_loss= 30.79200
0.518763793791758
Epoch: 0110 train_loss= 30.79199
Epoch: 0120 train_loss= 30.79199
Epoch: 0130 train_loss= 30.79198
Epoch: 0140 train_loss= 30.79198
Epoch: 0150 train_loss= 30.79198
Epoch: 0160 train_loss= 30.79198
Epoch: 0170 train_loss= 30.79198
Epoch: 0180 train_loss= 30.79198
Epoch: 0190 train_loss= 30.79198
Epoch: 0200 train_loss= 30.79198
0.5189040855987165
Epoch: 0210 train_loss= 30.79198
Epoch: 0220 train_loss= 30.79198
Epoch: 0230 train_loss= 30.79198
Epoch: 0240 train_loss= 30.79198
Epoch: 0250 train_loss= 30.79198
Epoch: 0260 train_loss= 30.79198
Epoch: 0270 train_loss= 30.79198
Epoch: 0280 train_loss= 30.79198
Epoch: 0290 train_loss= 30.79198
Epoch: 0300 train_loss= 30.79439
0.5217472701160248


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 32.57788
Epoch: 0020 train_loss= nan
Epoch: 0030 train_loss= nan
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu065.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu065.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 84, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 255, in _binary_roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 505, in roc_curve
    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 301, in _binary_clf_curve
    assert_all_finite(y_score)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 65, in assert_all_finite
    _assert_all_finite(X.data if sp.issparse(X) else X)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/utils/validation.py", line 58, in _assert_all_finite
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 39.30301
Epoch: 0020 train_loss= 39.20306
Epoch: 0030 train_loss= 38.90111
Epoch: 0040 train_loss= 38.82095
Epoch: 0050 train_loss= 38.81541
Epoch: 0060 train_loss= 38.80943
Epoch: 0070 train_loss= 38.80824
Epoch: 0080 train_loss= 38.80766
Epoch: 0090 train_loss= 38.80730
Epoch: 0100 train_loss= 38.80714
0.8390824023340213
Epoch: 0110 train_loss= 38.80698
Epoch: 0120 train_loss= 38.80681
Epoch: 0130 train_loss= 38.80666
Epoch: 0140 train_loss= 38.80647
Epoch: 0150 train_loss= 38.80628
Epoch: 0160 train_loss= 38.80608
Epoch: 0170 train_loss= 38.80586
Epoch: 0180 train_loss= 38.80562
Epoch: 0190 train_loss= 38.80536
Epoch: 0200 train_loss= 38.80507
0.845367252965001
Epoch: 0210 train_loss= 38.80474
Epoch: 0220 train_loss= 38.80452
Epoch: 0230 train_loss= 38.80423
Epoch: 0240 train_loss= 38.80399
Epoch: 0250 train_loss= 38.80371
Epoch: 0260 train_loss= 38.80333
Epoch: 0270 train_loss= 38.80295
Epoch: 0280 train_loss= 38.80330
Epoch: 0290 train_loss= 38.80248
Epoch: 0300 train_loss= 38.80221
0.8465865381983758


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 73913.15625
Epoch: 0020 train_loss= 73915.85938
Epoch: 0030 train_loss= 73912.99219
Epoch: 0040 train_loss= 73909.85938
Epoch: 0050 train_loss= 73909.85156
Epoch: 0060 train_loss= 73910.45312
Epoch: 0070 train_loss= 73912.19531
Epoch: 0080 train_loss= 73910.25000
Epoch: 0090 train_loss= 73911.35156
Epoch: 0100 train_loss= 73909.58594
0.6258478931140802
Epoch: 0110 train_loss= 73910.67969
Epoch: 0120 train_loss= 73909.59375
Epoch: 0130 train_loss= 73910.11719
Epoch: 0140 train_loss= 73909.89844
Epoch: 0150 train_loss= 73909.50000
Epoch: 0160 train_loss= 73910.34375
Epoch: 0170 train_loss= 73909.51562
Epoch: 0180 train_loss= 73909.66406
Epoch: 0190 train_loss= 73911.81250
Epoch: 0200 train_loss= 73909.46094
0.626901336073998
Epoch: 0210 train_loss= 73909.49219
Epoch: 0220 train_loss= 73911.71094
Epoch: 0230 train_loss= 73914.27344
Epoch: 0240 train_loss= 73909.53906
Epoch: 0250 train_loss= 73915.14844
Epoch: 0260 train_loss= 73909.82812
Epoch: 0270 train_loss= 73909.91406
Epoch: 0280 train_loss= 73909.53125
Epoch: 0290 train_loss= 73911.47656
Epoch: 0300 train_loss= 73909.56250
0.6274280575539569


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 22.09559
Epoch: 0020 train_loss= 22.09487
Epoch: 0030 train_loss= 22.09485
Epoch: 0040 train_loss= 22.09484
Epoch: 0050 train_loss= 22.09484
Epoch: 0060 train_loss= 22.09484
Epoch: 0070 train_loss= 22.09484
Epoch: 0080 train_loss= 22.09484
Epoch: 0090 train_loss= 22.09483
Epoch: 0100 train_loss= 22.09483
0.7924611744007278
Epoch: 0110 train_loss= 22.09482
Epoch: 0120 train_loss= 22.09481
Epoch: 0130 train_loss= 22.09478
Epoch: 0140 train_loss= 22.09476
Epoch: 0150 train_loss= 22.09475
Epoch: 0160 train_loss= 22.09472
Epoch: 0170 train_loss= 22.09471
Epoch: 0180 train_loss= 22.09468
Epoch: 0190 train_loss= 22.09466
Epoch: 0200 train_loss= 22.09464
0.792332714900754
Epoch: 0210 train_loss= 22.09461
Epoch: 0220 train_loss= 22.09460
Epoch: 0230 train_loss= 22.09466
Epoch: 0240 train_loss= 22.09463
Epoch: 0250 train_loss= 22.09457
Epoch: 0260 train_loss= 22.09455
Epoch: 0270 train_loss= 22.09453
Epoch: 0280 train_loss= 22.09452
Epoch: 0290 train_loss= 22.09451
Epoch: 0300 train_loss= 22.09450
0.792297773916761


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 4568.60791
Epoch: 0020 train_loss= 4552.54736
Epoch: 0030 train_loss= 4489.70898
Epoch: 0040 train_loss= 4417.41016
Epoch: 0050 train_loss= 4385.16895
Epoch: 0060 train_loss= 4362.61768
Epoch: 0070 train_loss= 4332.22656
Epoch: 0080 train_loss= 4296.43457
Epoch: 0090 train_loss= 4280.49609
Epoch: 0100 train_loss= 4269.27393
0.5734463276836158
Epoch: 0110 train_loss= 4287.74365
Epoch: 0120 train_loss= 4291.18164
Epoch: 0130 train_loss= 4300.45850
Epoch: 0140 train_loss= 4257.66357
Epoch: 0150 train_loss= 4243.95898
Epoch: 0160 train_loss= 4245.36182
Epoch: 0170 train_loss= 4250.81543
Epoch: 0180 train_loss= 4244.90723
Epoch: 0190 train_loss= 4241.84375
Epoch: 0200 train_loss= 4233.47998
0.5564971751412429
Epoch: 0210 train_loss= 4233.52979
Epoch: 0220 train_loss= 4229.08105
Epoch: 0230 train_loss= 4234.74609
Epoch: 0240 train_loss= 4220.96680
Epoch: 0250 train_loss= 4227.52539
Epoch: 0260 train_loss= 4229.89697
Epoch: 0270 train_loss= 4209.04150
Epoch: 0280 train_loss= 4213.59326
Epoch: 0290 train_loss= 4208.97900
Epoch: 0300 train_loss= 4194.81592
0.5338983050847458


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 453727296.00000
Epoch: 0020 train_loss= 453688896.00000
Epoch: 0030 train_loss= 453683648.00000
Epoch: 0040 train_loss= 453701568.00000
Epoch: 0050 train_loss= 453683552.00000
Epoch: 0060 train_loss= 453683136.00000
Epoch: 0070 train_loss= 453690464.00000
Epoch: 0080 train_loss= 453682112.00000
Epoch: 0090 train_loss= 453681536.00000
Epoch: 0100 train_loss= 453681696.00000
0.6903015966883501
Epoch: 0110 train_loss= 453681952.00000
Epoch: 0120 train_loss= 453682176.00000
Epoch: 0130 train_loss= 453681792.00000
Epoch: 0140 train_loss= 453680672.00000
Epoch: 0150 train_loss= 453721120.00000
Epoch: 0160 train_loss= 453681408.00000
Epoch: 0170 train_loss= 453682976.00000
Epoch: 0180 train_loss= 453683488.00000
Epoch: 0190 train_loss= 453689856.00000
Epoch: 0200 train_loss= 453688096.00000
0.6890745121230042
Epoch: 0210 train_loss= 453680992.00000
Epoch: 0220 train_loss= 453679392.00000
Epoch: 0230 train_loss= 453678592.00000
Epoch: 0240 train_loss= 453678432.00000
Epoch: 0250 train_loss= 453678592.00000
Epoch: 0260 train_loss= 453677952.00000
Epoch: 0270 train_loss= 453678336.00000
Epoch: 0280 train_loss= 453678208.00000
Epoch: 0290 train_loss= 453679808.00000
Epoch: 0300 train_loss= 453681856.00000
0.689340626848019


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= nan
Epoch: 0020 train_loss= nan
Epoch: 0030 train_loss= nan
Epoch: 0040 train_loss= nan
Epoch: 0050 train_loss= nan
Epoch: 0060 train_loss= nan
Epoch: 0070 train_loss= nan
Epoch: 0080 train_loss= nan
Epoch: 0090 train_loss= nan
Epoch: 0100 train_loss= nan
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
loss= 26.78261
Epoch: 0020 train_loss= 26.66107
Epoch: 0030 train_loss= 26.55589
Epoch: 0040 train_loss= 26.53548
Epoch: 0050 train_loss= 26.50541
Epoch: 0060 train_loss= 26.50417
Epoch: 0070 train_loss= 26.50282
Epoch: 0080 train_loss= 26.50229
Epoch: 0090 train_loss= 26.50217
Epoch: 0100 train_loss= 26.50210
0.4922963477680107
Epoch: 0110 train_loss= 26.50208
Epoch: 0120 train_loss= 26.50207
Epoch: 0130 train_loss= 26.50207
Epoch: 0140 train_loss= 26.50207
Epoch: 0150 train_loss= 26.50207
Epoch: 0160 train_loss= 26.50207
Epoch: 0170 train_loss= 26.50207
Epoch: 0180 train_loss= 26.50207
Epoch: 0190 train_loss= 26.50207
Epoch: 0200 train_loss= 26.50207
0.4921505635525024
Epoch: 0210 train_loss= 26.50207
Epoch: 0220 train_loss= 26.50207
Epoch: 0230 train_loss= 26.50207
Epoch: 0240 train_loss= 26.50207
Epoch: 0250 train_loss= 26.50207
Epoch: 0260 train_loss= 26.50207
Epoch: 0270 train_loss= 26.50207
Epoch: 0280 train_loss= 26.50207
Epoch: 0290 train_loss= 26.50207
Epoch: 0300 train_loss= 26.50207
0.4921502497005853


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 28.92984
Epoch: 0020 train_loss= 28.92344
Epoch: 0030 train_loss= 28.92250
Epoch: 0040 train_loss= 28.92245
Epoch: 0050 train_loss= 28.92238
Epoch: 0060 train_loss= 28.92236
Epoch: 0070 train_loss= 28.92234
Epoch: 0080 train_loss= 28.92233
Epoch: 0090 train_loss= 28.92237
Epoch: 0100 train_loss= 28.92334

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu015.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu015.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 33.01857
Epoch: 0020 train_loss= 32.59770
Epoch: 0030 train_loss= 32.49375
Epoch: 0040 train_loss= 32.46499
Epoch: 0050 train_loss= 32.46132
Epoch: 0060 train_loss= 32.45858
Epoch: 0070 train_loss= 32.45735
Epoch: 0080 train_loss= 32.45700
Epoch: 0090 train_loss= 32.45668
Epoch: 0100 train_loss= 32.45632
0.84786087706009
Epoch: 0110 train_loss= 32.45603
Epoch: 0120 train_loss= 32.45571
Epoch: 0130 train_loss= 32.45538
Epoch: 0140 train_loss= 32.45506
Epoch: 0150 train_loss= 32.45479
Epoch: 0160 train_loss= 32.45456
Epoch: 0170 train_loss= 32.45440
Epoch: 0180 train_loss= 32.45425
Epoch: 0190 train_loss= 32.45412
Epoch: 0200 train_loss= 32.45400
0.8512105166535647
Epoch: 0210 train_loss= 32.45388
Epoch: 0220 train_loss= 32.45375
Epoch: 0230 train_loss= 32.45362
Epoch: 0240 train_loss= 32.45349
Epoch: 0250 train_loss= 32.45333
Epoch: 0260 train_loss= 32.45317
Epoch: 0270 train_loss= 32.45301
Epoch: 0280 train_loss= 32.45283
Epoch: 0290 train_loss= 32.45264
Epoch: 0300 train_loss= 32.45245
0.851520048573511


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 92531.34375
Epoch: 0020 train_loss= 92377.44531
Epoch: 0030 train_loss= 92377.47656
Epoch: 0040 train_loss= 92377.57812
Epoch: 0050 train_loss= 92377.64062
Epoch: 0060 train_loss= 92378.51562
Epoch: 0070 train_loss= 92377.88281
Epoch: 0080 train_loss= 92377.50000
Epoch: 0090 train_loss= 92379.00781
Epoch: 0100 train_loss= 92377.51562
0.6261048304213772
Epoch: 0110 train_loss= 92378.95312
Epoch: 0120 train_loss= 92377.59375
Epoch: 0130 train_loss= 92377.63281
Epoch: 0140 train_loss= 92377.56250
Epoch: 0150 train_loss= 92377.51562
Epoch: 0160 train_loss= 92377.46094
Epoch: 0170 train_loss= 92377.97656
Epoch: 0180 train_loss= 92377.51562
Epoch: 0190 train_loss= 92377.50781
Epoch: 0200 train_loss= 92377.51562
0.6271325796505652
Epoch: 0210 train_loss= 92377.50000
Epoch: 0220 train_loss= 92377.49219
Epoch: 0230 train_loss= 92377.51562
Epoch: 0240 train_loss= 92378.04688
Epoch: 0250 train_loss= 92377.73438
Epoch: 0260 train_loss= 92377.72656
Epoch: 0270 train_loss= 92378.41406
Epoch: 0280 train_loss= 92378.12500
Epoch: 0290 train_loss= 92377.60938
Epoch: 0300 train_loss= 92377.51562
0.6254110996916753


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 18.60856
Epoch: 0020 train_loss= 18.60770
Epoch: 0030 train_loss= 18.60760
Epoch: 0040 train_loss= 18.60754
Epoch: 0050 train_loss= 18.60742
Epoch: 0060 train_loss= 18.60734
Epoch: 0070 train_loss= 18.60730
Epoch: 0080 train_loss= 18.60741
Epoch: 0090 train_loss= 18.60739
Epoch: 0100 train_loss= 18.60732
0.7881829592135949
Epoch: 0110 train_loss= 18.60725
Epoch: 0120 train_loss= 18.60721
Epoch: 0130 train_loss= 18.60720
Epoch: 0140 train_loss= 18.60718
Epoch: 0150 train_loss= 18.60719
Epoch: 0160 train_loss= 18.60715
Epoch: 0170 train_loss= 18.60713
Epoch: 0180 train_loss= 18.60714
Epoch: 0190 train_loss= 18.60716
Epoch: 0200 train_loss= 18.60711
0.788039084573624
Epoch: 0210 train_loss= 18.60710
Epoch: 0220 train_loss= 18.60709
Epoch: 0230 train_loss= 18.60715
Epoch: 0240 train_loss= 18.60708
Epoch: 0250 train_loss= 18.60707
Epoch: 0260 train_loss= 18.60705
Epoch: 0270 train_loss= 18.60704
Epoch: 0280 train_loss= 18.60705
Epoch: 0290 train_loss= 18.60702
Epoch: 0300 train_loss= 18.60701
0.787971943074971


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 5689.28564
Epoch: 0020 train_loss= 5663.71338
Epoch: 0030 train_loss= 5584.63672
Epoch: 0040 train_loss= 5492.99316
Epoch: 0050 train_loss= 5444.29199
Epoch: 0060 train_loss= 5388.61279
Epoch: 0070 train_loss= 5344.83545
Epoch: 0080 train_loss= 5313.91016
Epoch: 0090 train_loss= 5314.04150
Epoch: 0100 train_loss= 5282.45947
0.557909604519774
Epoch: 0110 train_loss= 5288.68848
Epoch: 0120 train_loss= 5269.51709
Epoch: 0130 train_loss= 5271.83301
Epoch: 0140 train_loss= 5263.22070
Epoch: 0150 train_loss= 5262.56152
Epoch: 0160 train_loss= 5250.55420
Epoch: 0170 train_loss= 5255.02344
Epoch: 0180 train_loss= 5263.18408
Epoch: 0190 train_loss= 5257.94873
Epoch: 0200 train_loss= 5241.22461
0.5155367231638418
Epoch: 0210 train_loss= 5260.66162
Epoch: 0220 train_loss= 5228.36719
Epoch: 0230 train_loss= 5229.23730
Epoch: 0240 train_loss= 5240.17480
Epoch: 0250 train_loss= 5204.97607
Epoch: 0260 train_loss= 5214.16895
Epoch: 0270 train_loss= 5206.87354
Epoch: 0280 train_loss= 5211.87744
Epoch: 0290 train_loss= 5199.63477
Epoch: 0300 train_loss= 5185.26758
0.5268361581920904


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 567363136.00000
Epoch: 0020 train_loss= 567138112.00000
Epoch: 0030 train_loss= 567155072.00000
Epoch: 0040 train_loss= 567103936.00000
Epoch: 0050 train_loss= 567103040.00000
Epoch: 0060 train_loss= 567103360.00000
Epoch: 0070 train_loss= 567109056.00000
Epoch: 0080 train_loss= 567102784.00000
Epoch: 0090 train_loss= 567103488.00000
Epoch: 0100 train_loss= 567104512.00000
0.69081904198699
Epoch: 0110 train_loss= 567103488.00000
Epoch: 0120 train_loss= 567103488.00000
Epoch: 0130 train_loss= 567103168.00000
Epoch: 0140 train_loss= 567103168.00000
Epoch: 0150 train_loss= 567102720.00000
Epoch: 0160 train_loss= 567102784.00000
Epoch: 0170 train_loss= 567102848.00000
Epoch: 0180 train_loss= 567102464.00000
Epoch: 0190 train_loss= 567101888.00000
Epoch: 0200 train_loss= 567102080.00000
0.6900059136605559
Epoch: 0210 train_loss= 567105984.00000
Epoch: 0220 train_loss= 567100992.00000
Epoch: 0230 train_loss= 567102400.00000
Epoch: 0240 train_loss= 567101376.00000
Epoch: 0250 train_loss= 567099712.00000
Epoch: 0260 train_loss= 567099712.00000
Epoch: 0270 train_loss= 567105664.00000
Epoch: 0280 train_loss= 567100608.00000
Epoch: 0290 train_loss= 567139072.00000
Epoch: 0300 train_loss= 567177088.00000
0.6938941454760497


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 22.34909
Epoch: 0020 train_loss= 22.34906
Epoch: 0030 train_loss= 22.34906
Epoch: 0040 train_loss= 22.34900
Epoch: 0050 train_loss= 22.34900
Epoch: 0060 train_loss= 22.34900
Epoch: 0070 train_loss= 22.34899
Epoch: 0080 train_loss= 22.34899
Epoch: 0090 train_loss= 22.34899
Epoch: 0100 train_loss= 22.34899
0.756311045274753
Epoch: 0110 train_loss= 22.34901
Epoch: 0120 train_loss= 22.34899
Epoch: 0130 train_loss= 22.34898
Epoch: 0140 train_loss= 22.34898
Epoch: 0150 train_loss= 22.34898
Epoch: 0160 train_loss= 22.34897
Epoch: 0170 train_loss= 22.34896
Epoch: 0180 train_loss= 22.34900
Epoch: 0190 traTraceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
in_loss= 22.34893
Epoch: 0200 train_loss= 22.34891
0.7559203870337393
Epoch: 0210 train_loss= 22.34889
Epoch: 0220 train_loss= 22.34888
Epoch: 0230 train_loss= 22.34886
Epoch: 0240 train_loss= 22.34883
Epoch: 0250 train_loss= 22.34878
Epoch: 0260 train_loss= 22.34867
Epoch: 0270 train_loss= 22.34853
Epoch: 0280 train_loss= 22.34849
Epoch: 0290 train_loss= 22.34854
Epoch: 0300 train_loss= 22.34826
0.7550892730510423


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 22.53075
Epoch: 0020 train_loss= 22.36973
Epoch: 0030 train_loss= 22.33371
Epoch: 0040 train_loss= 22.32624
Epoch: 0050 train_loss= 22.32643
Epoch: 0060 train_loss= 22.32579
Epoch: 0070 train_loss= 22.32576
Epoch: 0080 train_loss= 22.32569
Epoch: 0090 train_loss= 22.32569
Epoch: 0100 train_loss= 22.32568
0.5048949914255656
Epoch: 0110 train_loss= 22.32568
Epoch: 0120 train_loss= 22.32568
Epoch: 0130 train_loss= 22.32568
Epoch: 0140 train_loss= 22.32568
Epoch: 0150 train_loss= 22.32568
Epoch: 0160 train_loss= 22.32568
Epoch: 0170 train_loss= 22.32568
Epoch: 0180 train_loss= 22.32568
Epoch: 0190 train_loss= 22.32568
Epoch: 0200 train_loss= 22.32568
0.504855759935924
Epoch: 0210 train_loss= 22.32568
Epoch: 0220 train_loss= 22.32568
Epoch: 0230 train_loss= 22.32568
Epoch: 0240 train_loss= 22.32568
Epoch: 0250 train_loss= 22.32568
Epoch: 0260 train_loss= 22.32568
Epoch: 0270 train_loss= 22.32568
Epoch: 0280 train_loss= 22.32568
Epoch: 0290 train_loss= 22.32568
Epoch: 0300 train_loss= 22.32568
0.5048552891580483


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 25.28509
Epoch: 0020 train_loss= 25.27509
Epoch: 0030 train_loss= 25.27405
Epoch: 0040 train_loss= 25.27406
Epoch: 0050 train_loss= 25.27394
Epoch: 0060 train_loss= 25.27375
Epoch: 0070 train_loss= 25.27369
Epoch: 0080 train_loss= 25.27363
Epoch: 0090 train_loss= 25.27356
Epoch: 0100 train_loss= 25.27344

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu043.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu043.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 26.82695
Epoch: 0020 train_loss= 26.40319
Epoch: 0030 train_loss= 26.05525
Epoch: 0040 train_loss= 26.02534
Epoch: 0050 train_loss= 26.02226
Epoch: 0060 train_loss= 26.01775
Epoch: 0070 train_loss= 26.01575
Epoch: 0080 train_loss= 26.01449
Epoch: 0090 train_loss= 26.01355
Epoch: 0100 train_loss= 26.01289
0.8725239025380249
Epoch: 0110 train_loss= 26.01232
Epoch: 0120 train_loss= 26.01179
Epoch: 0130 train_loss= 26.01131
Epoch: 0140 train_loss= 26.01095
Epoch: 0150 train_loss= 26.01037
Epoch: 0160 train_loss= 26.00990
Epoch: 0170 train_loss= 26.00951
Epoch: 0180 train_loss= 26.00912
Epoch: 0190 train_loss= 26.00874
Epoch: 0200 train_loss= 26.00849
0.8686671484999519
Epoch: 0210 train_loss= 26.00887
Epoch: 0220 train_loss= 26.00754
Epoch: 0230 train_loss= 26.00723
Epoch: 0240 train_loss= 26.00669
Epoch: 0250 train_loss= 26.00622
Epoch: 0260 train_loss= 26.00578
Epoch: 0270 train_loss= 26.00534
Epoch: 0280 train_loss= 26.00492
Epoch: 0290 train_loss= 26.00451
Epoch: 0300 train_loss= 26.00414
0.8792882355343696


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 110899.74219
Epoch: 0020 train_loss= 110902.01562
Epoch: 0030 train_loss= 110866.79688
Epoch: 0040 train_loss= 110858.50781
Epoch: 0050 train_loss= 110851.35156
Epoch: 0060 train_loss= 110846.64844
Epoch: 0070 train_loss= 110856.33594
Epoch: 0080 train_loss= 110845.75000
Epoch: 0090 train_loss= 110845.56250
Epoch: 0100 train_loss= 110846.46875
0.624486125385406
Epoch: 0110 train_loss= 110845.48438
Epoch: 0120 train_loss= 110846.10156
Epoch: 0130 train_loss= 110845.50781
Epoch: 0140 train_loss= 110846.99219
Epoch: 0150 train_loss= 110847.13281
Epoch: 0160 train_loss= 110849.82031
Epoch: 0170 train_loss= 110846.47656
Epoch: 0180 train_loss= 110849.26562
Epoch: 0190 train_loss= 110845.86719
Epoch: 0200 train_loss= 110845.79688
0.6256166495375128
Epoch: 0210 train_loss= 110847.28906
Epoch: 0220 train_loss= 110845.57812
Epoch: 0230 train_loss= 110849.47656
Epoch: 0240 train_loss= 110845.43750
Epoch: 0250 train_loss= 110857.97656
Epoch: 0260 train_loss= 110846.47656
Epoch: 0270 train_loss= 110845.57812
Epoch: 0280 train_loss= 110856.50000
Epoch: 0290 train_loss= 110851.28125
Epoch: 0300 train_loss= 110845.38281
0.6251798561151078


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 15.12133
Epoch: 0020 train_loss= 15.12119
Epoch: 0030 train_loss= 15.12096
Epoch: 0040 train_loss= 15.12075
Epoch: 0050 train_loss= 15.12041
Epoch: 0060 train_loss= 15.12039
Epoch: 0070 train_loss= 15.12037
Epoch: 0080 train_loss= 15.12036
Epoch: 0090 train_loss= 15.12035
Epoch: 0100 train_loss= 15.12035
0.7851605640982074
Epoch: 0110 train_loss= 15.12035
Epoch: 0120 train_loss= 15.12035
Epoch: 0130 train_loss= 15.12035
Epoch: 0140 train_loss= 15.12035
Epoch: 0150 train_loss= 15.12035
Epoch: 0160 train_loss= 15.12035
Epoch: 0170 train_loss= 15.12035
Epoch: 0180 train_loss= 15.12035
Epoch: 0190 train_loss= 15.12035
Epoch: 0200 train_loss= 15.12035
0.7851643322435401
Epoch: 0210 train_loss= 15.12035
Epoch: 0220 train_loss= 15.12035
Epoch: 0230 train_loss= 15.12035
Epoch: 0240 train_loss= 15.12035
Epoch: 0250 train_loss= 15.12035
Epoch: 0260 train_loss= 15.12035
Epoch: 0270 train_loss= 15.12034
Epoch: 0280 train_loss= 15.12034
Epoch: 0290 train_loss= 15.12034
Epoch: 0300 train_loss= 15.12037
0.78521366069153


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 6838.43359
Epoch: 0020 train_loss= 6798.93408
Epoch: 0030 train_loss= 6735.17041
Epoch: 0040 train_loss= 6635.92188
Epoch: 0050 train_loss= 6536.67383
Epoch: 0060 train_loss= 6471.28174
Epoch: 0070 train_loss= 6411.87500
Epoch: 0080 train_loss= 6517.40234
Epoch: 0090 train_loss= 6374.10449
Epoch: 0100 train_loss= 6458.30322
0.5536723163841808
Epoch: 0110 train_loss= 6372.64795
Epoch: 0120 train_loss= 6350.74316
Epoch: 0130 train_loss= 6322.24854
Epoch: 0140 train_loss= 6359.03320
Epoch: 0150 train_loss= 6372.76709
Epoch: 0160 train_loss= 6339.23486
Epoch: 0170 train_loss= 6325.50488
Epoch: 0180 train_loss= 6300.58203
Epoch: 0190 train_loss= 6306.34961
Epoch: 0200 train_loss= 6296.88037
0.556497175141243
Epoch: 0210 train_loss= 6298.12256
Epoch: 0220 train_loss= 6278.15088
Epoch: 0230 train_loss= 6258.79443
Epoch: 0240 train_loss= 6288.73779
Epoch: 0250 train_loss= 6251.07764
Epoch: 0260 train_loss= 6247.53174
Epoch: 0270 train_loss= 6256.73633
Epoch: 0280 train_loss= 6215.96484
Epoch: 0290 train_loss= 6267.89307
Epoch: 0300 train_loss= 6239.60400
0.5296610169491526


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 680603840.00000
Epoch: 0020 train_loss= 680525760.00000
Epoch: 0030 train_loss= 680533568.00000
Epoch: 0040 train_loss= 680524992.00000
Epoch: 0050 train_loss= 680523904.00000
Epoch: 0060 train_loss= 680524032.00000
Epoch: 0070 train_loss= 680523776.00000
Epoch: 0080 train_loss= 680524416.00000
Epoch: 0090 train_loss= 680523968.00000
Epoch: 0100 train_loss= 680523584.00000
0.6904642223536369
Epoch: 0110 train_loss= 680534976.00000
Epoch: 0120 train_loss= 680526272.00000
Epoch: 0130 train_loss= 680525760.00000
Epoch: 0140 train_loss= 680558912.00000
Epoch: 0150 train_loss= 680547840.00000
Epoch: 0160 train_loss= 680532288.00000
Epoch: 0170 train_loss= 680522304.00000
Epoch: 0180 train_loss= 680584128.00000
Epoch: 0190 train_loss= 680520512.00000
Epoch: 0200 train_loss= 680548160.00000
0.6881431105854524
Epoch: 0210 train_loss= 680527104.00000
Epoch: 0220 train_loss= 680553600.00000
Epoch: 0230 train_loss= 680522048.00000
Epoch: 0240 train_loss= 680528896.00000
Epoch: 0250 train_loss= 680524992.00000
Epoch: 0260 train_loss= 680518656.00000
Epoch: 0270 train_loss= 680521088.00000
Epoch: 0280 train_loss= 680519552.00000
Epoch: 0290 train_loss= 680517888.00000
Epoch: 0300 train_loss= 680518144.00000
0.6900946185688942


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 18.11547
Epoch: 0020 train_loss= 18.11541
Epoch: 0030 train_loss= 18.11541
Epoch: 0040 train_loss= 18.11540
Epoch: 0050 train_loss= 18.11539
Epoch: 0060 train_loss= 18.11539
Epoch: 0070 train_loss= 18.11539
Epoch: 0080 train_loss= 18.11538
Epoch: 0090 train_loss= 18.11536
Epoch: 0100 train_loss= 18.11555
0.7537554564508249
Epoch: 0110 train_loss= 18.11543
Epoch: 0120 train_loss= 18.11540
Epoch: 0130 train_loss= 18.11537
Epoch: 0140 train_loss= 18.11534
Epoch: 0150 train_loss= 18.11530
Epoch: 0160 train_loss= 18.11526
Epoch: 0170 train_loss= 18.11520
Epoch: 0180 tTraceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
rain_loss= 18.11510
Epoch: 0190 train_loss= 18.11502
Epoch: 0200 train_loss= 18.11493
0.7540815040105898
Epoch: 0210 train_loss= 18.11482
Epoch: 0220 train_loss= 18.11467
Epoch: 0230 train_loss= 18.11500
Epoch: 0240 train_loss= 18.11466
Epoch: 0250 train_loss= 18.11449
Epoch: 0260 train_loss= 18.11444
Epoch: 0270 train_loss= 18.11515
Epoch: 0280 train_loss= 18.11504
Epoch: 0290 train_loss= 18.11492
Epoch: 0300 train_loss= 18.11482
0.7540257182028776


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 18.40840
Epoch: 0020 train_loss= 18.26968
Epoch: 0030 train_loss= 18.09557
Epoch: 0040 train_loss= 18.04410
Epoch: 0050 train_loss= 18.03956
Epoch: 0060 train_loss= 18.03811
Epoch: 0070 train_loss= 18.03803
Epoch: 0080 train_loss= 18.03793
Epoch: 0090 train_loss= 18.03790
Epoch: 0100 train_loss= 18.03789
0.48777766479108764
Epoch: 0110 train_loss= 18.03789
Epoch: 0120 train_loss= 18.03790
Epoch: 0130 train_loss= 18.03789
Epoch: 0140 train_loss= 18.03790
Epoch: 0150 train_loss= 18.03791
Epoch: 0160 train_loss= 18.03790
Epoch: 0170 train_loss= 18.03789
Epoch: 0180 train_loss= 18.03789
Epoch: 0190 train_loss= 18.03789
Epoch: 0200 train_loss= 18.03790
0.48789834085322525
Epoch: 0210 train_loss= 18.03824
Epoch: 0220 train_loss= 18.03796
Epoch: 0230 train_loss= 18.03790
Epoch: 0240 train_loss= 18.03789
Epoch: 0250 train_loss= 18.03789
Epoch: 0260 train_loss= 18.03789
Epoch: 0270 train_loss= 18.03789
Epoch: 0280 train_loss= 18.03789
Epoch: 0290 train_loss= 18.04330
Epoch: 0300 train_loss= 18.04445
0.4861782754213776


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 21.63591
Epoch: 0020 train_loss= 21.62663
Epoch: 0030 train_loss= 21.62544
Epoch: 0040 train_loss= 21.62521
Epoch: 0050 train_loss= 21.62518
Epoch: 0060 train_loss= 21.62514
Epoch: 0070 train_loss= 21.62524
Epoch: 0080 train_loss= 21.62506
Epoch: 0090 train_loss= 21.62497
Epoch: 0100 train_loss= 21.62494

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu036.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu036.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 20.27312
Epoch: 0020 train_loss= 20.11441
Epoch: 0030 train_loss= 20.09279
Epoch: 0040 train_loss= 20.08833
Epoch: 0050 train_loss= 20.08694
Epoch: 0060 train_loss= 20.08636
Epoch: 0070 train_loss= 20.08613
Epoch: 0080 train_loss= 20.08598
Epoch: 0090 train_loss= 20.08587
Epoch: 0100 train_loss= 20.08577
0.7752005088934242
Epoch: 0110 train_loss= 20.08566
Epoch: 0120 train_loss= 20.08556
Epoch: 0130 train_loss= 20.08546
Epoch: 0140 train_loss= 20.08536
Epoch: 0150 train_loss= 20.08526
Epoch: 0160 train_loss= 20.08516
Epoch: 0170 train_loss= 20.08505
Epoch: 0180 train_loss= 20.08495
Epoch: 0190 train_loss= 20.08484
Epoch: 0200 train_loss= 20.08472
0.7745867083067188
Epoch: 0210 train_loss= 20.08460
Epoch: 0220 train_loss= 20.08448
Epoch: 0230 train_loss= 20.08435
Epoch: 0240 train_loss= 20.08423
Epoch: 0250 train_loss= 20.08410
Epoch: 0260 train_loss= 20.08397
Epoch: 0270 train_loss= 20.08385
Epoch: 0280 train_loss= 20.08372
Epoch: 0290 train_loss= 20.08360
Epoch: 0300 train_loss= 20.08348
0.7754022493880942


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 129316.03125
Epoch: 0020 train_loss= 129313.75000
Epoch: 0030 train_loss= 129313.47656
Epoch: 0040 train_loss= 129313.45312
Epoch: 0050 train_loss= 129313.56250
Epoch: 0060 train_loss= 129313.44531
Epoch: 0070 train_loss= 129315.63281
Epoch: 0080 train_loss= 129313.55469
Epoch: 0090 train_loss= 129315.34375
Epoch: 0100 train_loss= 129314.29688
0.6238566289825282
Epoch: 0110 train_loss= 129316.65625
Epoch: 0120 train_loss= 129313.42188
Epoch: 0130 train_loss= 129315.21094
Epoch: 0140 train_loss= 129314.27344
Epoch: 0150 train_loss= 129313.40625
Epoch: 0160 train_loss= 129314.81250
Epoch: 0170 train_loss= 129313.51562
Epoch: 0180 train_loss= 129315.20312
Epoch: 0190 train_loss= 129314.32812
Epoch: 0200 train_loss= 129313.42969
0.6256166495375128
Epoch: 0210 train_loss= 129314.26562
Epoch: 0220 train_loss= 129316.50781
Epoch: 0230 train_loss= 129315.21094
Epoch: 0240 train_loss= 129317.00781
Epoch: 0250 train_loss= 129314.25000
Epoch: 0260 train_loss= 129313.53125
Epoch: 0270 train_loss= 129315.56250
Epoch: 0280 train_loss= 129314.41406
Epoch: 0290 train_loss= 129314.77344
Epoch: 0300 train_loss= 129313.61719
0.6260534429599178


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 11.63417
Epoch: 0020 train_loss= 11.63370
Epoch: 0030 train_loss= 11.63355
Epoch: 0040 train_loss= 11.63341
Epoch: 0050 train_loss= 11.63329
Epoch: 0060 train_loss= 11.63337
Epoch: 0070 train_loss= 11.63339
Epoch: 0080 train_loss= 11.63319
Epoch: 0090 train_loss= 11.63311
Epoch: 0100 train_loss= 11.63305
0.783041496186637
Epoch: 0110 train_loss= 11.63303
Epoch: 0120 train_loss= 11.63301
Epoch: 0130 train_loss= 11.63290
Epoch: 0140 train_loss= 11.63298
Epoch: 0150 train_loss= 11.63280
Epoch: 0160 train_loss= 11.63273
Epoch: 0170 train_loss= 11.63267
Epoch: 0180 train_loss= 11.63257
Epoch: 0190 train_loss= 11.63252
Epoch: 0200 train_loss= 11.63249
0.782879808496003
Epoch: 0210 train_loss= 11.63244
Epoch: 0220 train_loss= 11.63242
Epoch: 0230 train_loss= 11.63245
Epoch: 0240 train_loss= 11.63229
Epoch: 0250 train_loss= 11.63227
Epoch: 0260 train_loss= 11.63232
Epoch: 0270 train_loss= 11.63262
Epoch: 0280 train_loss= 11.63220
Epoch: 0290 train_loss= 11.63213
Epoch: 0300 train_loss= 11.63206
0.782909611099997


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 7949.69385
Epoch: 0020 train_loss= 7902.23193
Epoch: 0030 train_loss= 7794.97949
Epoch: 0040 train_loss= 7671.20117
Epoch: 0050 train_loss= 7611.73779
Epoch: 0060 train_loss= 7557.07910
Epoch: 0070 train_loss= 7457.86084
Epoch: 0080 train_loss= 7435.12695
Epoch: 0090 train_loss= 7426.95020
Epoch: 0100 train_loss= 7447.05371
0.557909604519774
Epoch: 0110 train_loss= 7404.84912
Epoch: 0120 train_loss= 7391.58691
Epoch: 0130 train_loss= 7387.58496
Epoch: 0140 train_loss= 7362.57617
Epoch: 0150 train_loss= 7338.79053
Epoch: 0160 train_loss= 7328.50586
Epoch: 0170 train_loss= 7334.81201
Epoch: 0180 train_loss= 7314.36182
Epoch: 0190 train_loss= 7372.91846
Epoch: 0200 train_loss= 7300.40674
0.5494350282485876
Epoch: 0210 train_loss= 7295.12012
Epoch: 0220 train_loss= 7269.68555
Epoch: 0230 train_loss= 7259.87744
Epoch: 0240 train_loss= 7245.68311
Epoch: 0250 train_loss= 7238.61475
Epoch: 0260 train_loss= 7224.40771
Epoch: 0270 train_loss= 7199.73047
Epoch: 0280 train_loss= 7187.79150
Epoch: 0290 train_loss= 7167.90723
Epoch: 0300 train_loss= 7149.90234
0.5437853107344632


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 794085696.00000
Epoch: 0020 train_loss= 794141888.00000
Epoch: 0030 train_loss= 793952832.00000
Epoch: 0040 train_loss= 793944896.00000
Epoch: 0050 train_loss= 793947904.00000
Epoch: 0060 train_loss= 793948352.00000
Epoch: 0070 train_loss= 793946752.00000
Epoch: 0080 train_loss= 793944000.00000
Epoch: 0090 train_loss= 793944704.00000
Epoch: 0100 train_loss= 793943872.00000
0.6906268480189237
Epoch: 0110 train_loss= 793944448.00000
Epoch: 0120 train_loss= 793943360.00000
Epoch: 0130 train_loss= 793943104.00000
Epoch: 0140 train_loss= 793942528.00000
Epoch: 0150 train_loss= 794465472.00000
Epoch: 0160 train_loss= 793944960.00000
Epoch: 0170 train_loss= 793951936.00000
Epoch: 0180 train_loss= 793947712.00000
Epoch: 0190 train_loss= 793946112.00000
Epoch: 0200 train_loss= 793945600.00000
0.6909077468953282
Epoch: 0210 train_loss= 793943872.00000
Epoch: 0220 train_loss= 793967296.00000
Epoch: 0230 train_loss= 793951168.00000
Epoch: 0240 train_loss= 793944256.00000
Epoch: 0250 train_loss= 793941376.00000
Epoch: 0260 train_loss= 793941184.00000
Epoch: 0270 train_loss= 794187136.00000
Epoch: 0280 train_loss= 793939584.00000
Epoch: 0290 train_loss= 793955776.00000
Epoch: 0300 train_loss= 793966080.00000
0.6881283264340627


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 13.88188
Epoch: 0020 train_loss= 13.88183
Epoch: 0030 train_loss= 13.88181
Epoch: 0040 train_loss= 13.88181
Epoch: 0050 train_loss= 13.88179
Epoch: 0060 train_loss= 13.88178
Epoch: 0070 train_loss= 13.88176
Epoch: 0080 train_loss= 13.88185
Epoch: 0090 train_loss= 13.88173
Epoch: 0100 train_loss= 13.88167
0.752191877964606
Epoch: 0110 train_loss= 13.88161
Epoch: 0120 train_loss= 13.88154
Epoch: 0130 train_loss= 13.88149
Epoch: 0140 train_loss= 13.88144
Epoch: 0150 train_loss= 13.88126
Epoch: 0160 train_loss= 13.88127
Epoch: 0170 train_loss= 13.88135
Epoch: 0180 trTraceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
ain_loss= 13.88116
Epoch: 0190 train_loss= 13.88101
Epoch: 0200 train_loss= 13.88063
0.7495830247254047
Epoch: 0210 train_loss= 13.88036
Epoch: 0220 train_loss= 13.88023
Epoch: 0230 train_loss= 13.87984
Epoch: 0240 train_loss= 13.87998
Epoch: 0250 train_loss= 13.87932
Epoch: 0260 train_loss= 13.87963
Epoch: 0270 train_loss= 13.87946
Epoch: 0280 train_loss= 13.87879
Epoch: 0290 train_loss= 13.87835
Epoch: 0300 train_loss= 13.87780
0.7480104637786218


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 14.09391
Epoch: 0020 train_loss= 13.84914
Epoch: 0030 train_loss= 13.81545
Epoch: 0040 train_loss= 13.81229
Epoch: 0050 train_loss= 13.81033
Epoch: 0060 train_loss= 13.81014
Epoch: 0070 train_loss= 13.80985
Epoch: 0080 train_loss= 13.80980
Epoch: 0090 train_loss= 13.80979
Epoch: 0100 train_loss= 13.80978
0.49122313113737426
Epoch: 0110 train_loss= 13.80978
Epoch: 0120 train_loss= 13.80978
Epoch: 0130 train_loss= 13.80978
Epoch: 0140 train_loss= 13.80978
Epoch: 0150 train_loss= 13.80978
Epoch: 0160 train_loss= 13.80978
Epoch: 0170 train_loss= 13.80978
Epoch: 0180 train_loss= 13.80978
Epoch: 0190 train_loss= 13.80978
Epoch: 0200 train_loss= 13.80978
0.4912403929928165
Epoch: 0210 train_loss= 13.80978
Epoch: 0220 train_loss= 13.80978
Epoch: 0230 train_loss= 13.80978
Epoch: 0240 train_loss= 13.80978
Epoch: 0250 train_loss= 13.80978
Epoch: 0260 train_loss= 13.80978
Epoch: 0270 train_loss= 13.80978
Epoch: 0280 train_loss= 13.80978
Epoch: 0290 train_loss= 13.80978
Epoch: 0300 train_loss= 13.80978
0.4912396083630237


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 17.98067
Epoch: 0020 train_loss= 17.97721
Epoch: 0030 train_loss= 17.97698
Epoch: 0040 train_loss= 17.97693
Epoch: 0050 train_loss= 17.97688
Epoch: 0060 train_loss= 17.97683
Epoch: 0070 train_loss= 17.97673
Epoch: 0080 train_loss= 17.97652
Epoch: 0090 train_loss= 17.97630
Epoch: 0100 train_loss= 17.97719

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu065.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu065.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 14.18504
Epoch: 0020 train_loss= 13.53612
Epoch: 0030 train_loss= 13.50405
Epoch: 0040 train_loss= 13.48835
Epoch: 0050 train_loss= 13.48712
Epoch: 0060 train_loss= 13.48555
Epoch: 0070 train_loss= 13.48507
Epoch: 0080 train_loss= 13.48476
Epoch: 0090 train_loss= 13.48451
Epoch: 0100 train_loss= 13.48424
0.8598210936029579
Epoch: 0110 train_loss= 13.48399
Epoch: 0120 train_loss= 13.48374
Epoch: 0130 train_loss= 13.48350
Epoch: 0140 train_loss= 13.48328
Epoch: 0150 train_loss= 13.48305
Epoch: 0160 train_loss= 13.48283
Epoch: 0170 train_loss= 13.48259
Epoch: 0180 train_loss= 13.48235
Epoch: 0190 train_loss= 13.48210
Epoch: 0200 train_loss= 13.48184
0.8564915070041268
Epoch: 0210 train_loss= 13.48158
Epoch: 0220 train_loss= 13.48123
Epoch: 0230 train_loss= 13.48078
Epoch: 0240 train_loss= 13.48026
Epoch: 0250 train_loss= 13.47977
Epoch: 0260 train_loss= 13.47935
Epoch: 0270 train_loss= 13.47904
Epoch: 0280 train_loss= 13.47876
Epoch: 0290 train_loss= 13.47851
Epoch: 0300 train_loss= 13.47827
0.8581738532529221


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 147892.81250
Epoch: 0020 train_loss= 147809.84375
Epoch: 0030 train_loss= 147784.64062
Epoch: 0040 train_loss= 147786.56250
Epoch: 0050 train_loss= 147781.37500
Epoch: 0060 train_loss= 147782.75000
Epoch: 0070 train_loss= 147782.64062
Epoch: 0080 train_loss= 147782.15625
Epoch: 0090 train_loss= 147783.35938
Epoch: 0100 train_loss= 147784.71875
0.6251927029804727
Epoch: 0110 train_loss= 147782.28125
Epoch: 0120 train_loss= 147783.56250
Epoch: 0130 train_loss= 147782.25000
Epoch: 0140 train_loss= 147786.12500
Epoch: 0150 train_loss= 147786.12500
Epoch: 0160 train_loss= 147783.48438
Epoch: 0170 train_loss= 147781.48438
Epoch: 0180 train_loss= 147782.57812
Epoch: 0190 train_loss= 147781.37500
Epoch: 0200 train_loss= 147783.03125
0.6266572456320658
Epoch: 0210 train_loss= 147781.37500
Epoch: 0220 train_loss= 147783.56250
Epoch: 0230 train_loss= 147781.50000
Epoch: 0240 train_loss= 147782.26562
Epoch: 0250 train_loss= 147782.12500
Epoch: 0260 train_loss= 147787.17188
Epoch: 0270 train_loss= 147790.17188
Epoch: 0280 train_loss= 147787.28125
Epoch: 0290 train_loss= 147781.46875
Epoch: 0300 train_loss= 147788.01562
0.6264131551901336


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 8.14801
Epoch: 0020 train_loss= 8.14741
Epoch: 0030 train_loss= 8.14720
Epoch: 0040 train_loss= 8.14716
Epoch: 0050 train_loss= 8.14715
Epoch: 0060 train_loss= 8.14714
Epoch: 0070 train_loss= 8.14714
Epoch: 0080 train_loss= 8.14714
Epoch: 0090 train_loss= 8.14713
Epoch: 0100 train_loss= 8.14713
0.7815314975842763
Epoch: 0110 train_loss= 8.14713
Epoch: 0120 train_loss= 8.14713
Epoch: 0130 train_loss= 8.14713
Epoch: 0140 train_loss= 8.14713
Epoch: 0150 train_loss= 8.14713
Epoch: 0160 train_loss= 8.14713
Epoch: 0170 train_loss= 8.14713
Epoch: 0180 train_loss= 8.14713
Epoch: 0190 train_loss= 8.14713
Epoch: 0200 train_loss= 8.14713
0.7815345806122757
Epoch: 0210 train_loss= 8.14713
Epoch: 0220 train_loss= 8.14713
Epoch: 0230 train_loss= 8.14713
Epoch: 0240 train_loss= 8.14713
Epoch: 0250 train_loss= 8.14713
Epoch: 0260 train_loss= 8.14713
Epoch: 0270 train_loss= 8.14713
Epoch: 0280 train_loss= 8.14713
Epoch: 0290 train_loss= 8.14713
Epoch: 0300 train_loss= 8.14713
0.7815345806122758


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 9269.82422
Epoch: 0020 train_loss= 9254.96973
Epoch: 0030 train_loss= 9252.85254
Epoch: 0040 train_loss= 9228.67871
Epoch: 0050 train_loss= 9249.54004
Epoch: 0060 train_loss= 9227.12012
Epoch: 0070 train_loss= 9228.93066
Epoch: 0080 train_loss= 9222.47656
Epoch: 0090 train_loss= 9229.33691
Epoch: 0100 train_loss= 9234.73633
0.4703389830508475
Epoch: 0110 train_loss= 9226.83691
Epoch: 0120 train_loss= 9224.20703
Epoch: 0130 train_loss= 9254.01758
Epoch: 0140 train_loss= 9222.75000
Epoch: 0150 train_loss= 9220.75000
Epoch: 0160 train_loss= 9217.50488
Epoch: 0170 train_loss= 9214.13184
Epoch: 0180 train_loss= 9210.84082
Epoch: 0190 train_loss= 9208.40234
Epoch: 0200 train_loss= 9205.75488
0.4731638418079096
Epoch: 0210 train_loss= 9204.40918
Epoch: 0220 train_loss= 9202.07129
Epoch: 0230 train_loss= 9200.77051
Epoch: 0240 train_loss= 9199.23633
Epoch: 0250 train_loss= 9199.58398
Epoch: 0260 train_loss= 9196.16504
Epoch: 0270 train_loss= 9206.87109
Epoch: 0280 train_loss= 9219.39355
Epoch: 0290 train_loss= 9204.64551
Epoch: 0300 train_loss= 9203.86621
0.4731638418079096


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 907434304.00000
Epoch: 0020 train_loss= 907434624.00000
Epoch: 0030 train_loss= 907366592.00000
Epoch: 0040 train_loss= 907365056.00000
Epoch: 0050 train_loss= 907369600.00000
Epoch: 0060 train_loss= 907373568.00000
Epoch: 0070 train_loss= 907365888.00000
Epoch: 0080 train_loss= 907364800.00000
Epoch: 0090 train_loss= 907367872.00000
Epoch: 0100 train_loss= 907365312.00000
0.6905529272619753
Epoch: 0110 train_loss= 907386176.00000
Epoch: 0120 train_loss= 907366912.00000
Epoch: 0130 train_loss= 907365568.00000
Epoch: 0140 train_loss= 907365888.00000
Epoch: 0150 train_loss= 907365504.00000
Epoch: 0160 train_loss= 907366848.00000
Epoch: 0170 train_loss= 907364032.00000
Epoch: 0180 train_loss= 907362752.00000
Epoch: 0190 train_loss= 907367744.00000
Epoch: 0200 train_loss= 907365824.00000
0.6910999408633945
Epoch: 0210 train_loss= 907360384.00000
Epoch: 0220 train_loss= 907366080.00000
Epoch: 0230 train_loss= 907370624.00000
Epoch: 0240 train_loss= 907363840.00000
Epoch: 0250 train_loss= 907361280.00000
Epoch: 0260 train_loss= 907360704.00000
Epoch: 0270 train_loss= 907361216.00000
Epoch: 0280 train_loss= 907360576.00000
Epoch: 0290 train_loss= 907360384.00000
Epoch: 0300 train_loss= 907360064.00000
0.6902424600827912


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 9.64829
Epoch: 0020 train_loss= 9.64827
Epoch: 0030 train_loss= 9.64829
Epoch: 0040 train_loss= 9.64821
Epoch: 0050 train_loss= 9.64820
Epoch: 0060 train_loss= 9.64819
Epoch: 0070 train_loss= 9.64818
Epoch: 0080 train_loss= 9.64839
Epoch: 0090 train_loss= 9.64831
Epoch: 0100 train_loss= 9.64826
0.7510925508612131
Epoch: 0110 train_loss= 9.64818
Epoch: 0120 train_loss= 9.64818
Epoch: 0130 train_loss= 9.64814
Epoch: 0140 train_loss= 9.64839
Epoch: 0150 train_loss= 9.64813
Epoch: 0160 train_loss= 9.64805
Epoch: 0170 train_loss= 9.64801
Epoch: 0180 train_loss= 9.64797
Epoch: 0190 train_loss= Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
9.64801
Epoch: 0200 train_loss= 9.64791
0.750577398868525
Epoch: 0210 train_loss= 9.64787
Epoch: 0220 train_loss= 9.64782
Epoch: 0230 train_loss= 9.64778
Epoch: 0240 train_loss= 9.64770
Epoch: 0250 train_loss= 9.64762
Epoch: 0260 train_loss= 9.64755
Epoch: 0270 train_loss= 9.64746
Epoch: 0280 train_loss= 9.64740
Epoch: 0290 train_loss= 9.64741
Epoch: 0300 train_loss= 9.64728
0.7491838567849094


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 9.91139
Epoch: 0020 train_loss= 9.72913
Epoch: 0030 train_loss= 9.66412
Epoch: 0040 train_loss= 9.60910
Epoch: 0050 train_loss= 9.54613
Epoch: 0060 train_loss= 9.54816
Epoch: 0070 train_loss= 9.54368
Epoch: 0080 train_loss= 9.54370
Epoch: 0090 train_loss= 9.54335
Epoch: 0100 train_loss= 9.54335
0.48668938326842875
Epoch: 0110 train_loss= 9.54331
Epoch: 0120 train_loss= 9.54331
Epoch: 0130 train_loss= 9.54330
Epoch: 0140 train_loss= 9.54330
Epoch: 0150 train_loss= 9.54330
Epoch: 0160 train_loss= 9.54330
Epoch: 0170 train_loss= 9.54330
Epoch: 0180 train_loss= 9.54330
Epoch: 0190 train_loss= 9.54330
Epoch: 0200 train_loss= 9.54330
0.48664575785194725
Epoch: 0210 train_loss= 9.54330
Epoch: 0220 train_loss= 9.54330
Epoch: 0230 train_loss= 9.54330
Epoch: 0240 train_loss= 9.54330
Epoch: 0250 train_loss= 9.54330
Epoch: 0260 train_loss= 9.54330
Epoch: 0270 train_loss= 9.54330
Epoch: 0280 train_loss= 9.54330
Epoch: 0290 train_loss= 9.54330
Epoch: 0300 train_loss= 9.54330
0.48664293318469304


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 14.33615
Epoch: 0020 train_loss= 14.32947
Epoch: 0030 train_loss= 14.32827
Epoch: 0040 train_loss= 14.32821
Epoch: 0050 train_loss= 14.32814
Epoch: 0060 train_loss= 14.32812
Epoch: 0070 train_loss= 14.32809
Epoch: 0080 train_loss= 14.32805
Epoch: 0090 train_loss= 14.32800
Epoch: 0100 train_loss= 14.32793

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu014.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 7.58310
Epoch: 0020 train_loss= 7.56225
Epoch: 0030 train_loss= 7.53958
Epoch: 0040 train_loss= 7.51307
Epoch: 0050 train_loss= 7.48890
Epoch: 0060 train_loss= 7.48320
Epoch: 0070 train_loss= 7.48337
Epoch: 0080 train_loss= 7.48223
Epoch: 0090 train_loss= 7.48206
Epoch: 0100 train_loss= 7.48189
0.7744353371450514
Epoch: 0110 train_loss= 7.48172
Epoch: 0120 train_loss= 7.48158
Epoch: 0130 train_loss= 7.48143
Epoch: 0140 train_loss= 7.48129
Epoch: 0150 train_loss= 7.48114
Epoch: 0160 train_loss= 7.48098
Epoch: 0170 train_loss= 7.48082
Epoch: 0180 train_loss= 7.48064
Epoch: 0190 train_loss= 7.48046
Epoch: 0200 train_loss= 7.48027
0.7774768658969404
Epoch: 0210 train_loss= 7.48007
Epoch: 0220 train_loss= 7.47986
Epoch: 0230 train_loss= 7.47962
Epoch: 0240 train_loss= 7.47938
Epoch: 0250 train_loss= 7.47914
Epoch: 0260 train_loss= 7.47891
Epoch: 0270 train_loss= 7.47870
Epoch: 0280 train_loss= 7.47849
Epoch: 0290 train_loss= 7.47830
Epoch: 0300 train_loss= 7.47812
0.7830201767863586


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 166287.54688
Epoch: 0020 train_loss= 166249.39062
Epoch: 0030 train_loss= 166305.60938
Epoch: 0040 train_loss= 166251.34375
Epoch: 0050 train_loss= 166255.00000
Epoch: 0060 train_loss= 166249.37500
Epoch: 0070 train_loss= 166249.45312
Epoch: 0080 train_loss= 166249.37500
Epoch: 0090 train_loss= 166249.40625
Epoch: 0100 train_loss= 166250.00000
0.6259763617677288
Epoch: 0110 train_loss= 166249.48438
Epoch: 0120 train_loss= 166249.48438
Epoch: 0130 train_loss= 166250.10938
Epoch: 0140 train_loss= 166249.82812
Epoch: 0150 train_loss= 166250.57812
Epoch: 0160 train_loss= 166250.10938
Epoch: 0170 train_loss= 166249.51562
Epoch: 0180 train_loss= 166249.50000
Epoch: 0190 train_loss= 166249.48438
Epoch: 0200 train_loss= 166250.50000
0.6261048304213772
Epoch: 0210 train_loss= 166249.35938
Epoch: 0220 train_loss= 166249.32812
Epoch: 0230 train_loss= 166250.10938
Epoch: 0240 train_loss= 166249.40625
Epoch: 0250 train_loss= 166249.78125
Epoch: 0260 train_loss= 166249.45312
Epoch: 0270 train_loss= 166249.40625
Epoch: 0280 train_loss= 166249.68750
Epoch: 0290 train_loss= 166253.28125
Epoch: 0300 train_loss= 166253.21875
0.6248586844809867


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 4.66077
Epoch: 0020 train_loss= 4.66027
Epoch: 0030 train_loss= 4.66015
Epoch: 0040 train_loss= 4.66006
Epoch: 0050 train_loss= 4.65993
Epoch: 0060 train_loss= 4.65982
Epoch: 0070 train_loss= 4.65971
Epoch: 0080 train_loss= 4.65957
Epoch: 0090 train_loss= 4.65942
Epoch: 0100 train_loss= 4.65949
0.7801872973765487
Epoch: 0110 train_loss= 4.65933
Epoch: 0120 train_loss= 4.65922
Epoch: 0130 train_loss= 4.65913
Epoch: 0140 train_loss= 4.65905
Epoch: 0150 train_loss= 4.65939
Epoch: 0160 train_loss= 4.65907
Epoch: 0170 train_loss= 4.65891
Epoch: 0180 train_loss= 4.65880
Epoch: 0190 train_loss= 4.65873
Epoch: 0200 train_loss= 4.65868
0.7800913809499015
Epoch: 0210 train_loss= 4.65861
Epoch: 0220 train_loss= 4.65856
Epoch: 0230 train_loss= 4.65846
Epoch: 0240 train_loss= 4.65844
Epoch: 0250 train_loss= 4.65833
Epoch: 0260 train_loss= 4.65830
Epoch: 0270 train_loss= 4.65820
Epoch: 0280 train_loss= 4.65823
Epoch: 0290 train_loss= 4.65812
Epoch: 0300 train_loss= 4.65810
0.7800896681565684


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 10283.91895
Epoch: 0020 train_loss= 10206.21094
Epoch: 0030 train_loss= 10057.89355
Epoch: 0040 train_loss= 9886.91504
Epoch: 0050 train_loss= 9954.25781
Epoch: 0060 train_loss= 9727.31152
Epoch: 0070 train_loss= 9643.88770
Epoch: 0080 train_loss= 9580.78125
Epoch: 0090 train_loss= 9563.96680
Epoch: 0100 train_loss= 9580.08301
0.5480225988700564
Epoch: 0110 train_loss= 9484.56543
Epoch: 0120 train_loss= 9609.67773
Epoch: 0130 train_loss= 9542.29980
Epoch: 0140 train_loss= 9486.36523
Epoch: 0150 train_loss= 9476.23828
Epoch: 0160 train_loss= 9421.47559
Epoch: 0170 train_loss= 9398.33789
Epoch: 0180 train_loss= 9410.31934
Epoch: 0190 train_loss= 9385.39844
Epoch: 0200 train_loss= 9358.05762
0.5607344632768361
Epoch: 0210 train_loss= 9345.28906
Epoch: 0220 train_loss= 9346.36426
Epoch: 0230 train_loss= 9338.81348
Epoch: 0240 train_loss= 9322.45508
Epoch: 0250 train_loss= 9373.49707
Epoch: 0260 train_loss= 9322.00488
Epoch: 0270 train_loss= 9285.94727
Epoch: 0280 train_loss= 9331.77734
Epoch: 0290 train_loss= 9426.18359
Epoch: 0300 train_loss= 9319.23633
0.5409604519774012


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 1020899968.00000
Epoch: 0020 train_loss= 1020902464.00000
Epoch: 0030 train_loss= 1020788672.00000
Epoch: 0040 train_loss= 1020787072.00000
Epoch: 0050 train_loss= 1020790976.00000
Epoch: 0060 train_loss= 1020818368.00000
Epoch: 0070 train_loss= 1020785920.00000
Epoch: 0080 train_loss= 1020785536.00000
Epoch: 0090 train_loss= 1020784640.00000
Epoch: 0100 train_loss= 1020784960.00000
0.6903163808397399
Epoch: 0110 train_loss= 1020784768.00000
Epoch: 0120 train_loss= 1020784640.00000
Epoch: 0130 train_loss= 1020783040.00000
Epoch: 0140 train_loss= 1020781632.00000
Epoch: 0150 train_loss= 1020778560.00000
Epoch: 0160 train_loss= 1020777600.00000
Epoch: 0170 train_loss= 1020779136.00000
Epoch: 0180 train_loss= 1020776448.00000
Epoch: 0190 train_loss= 1020777152.00000
Epoch: 0200 train_loss= 1020777152.00000
0.6904346540508575
Epoch: 0210 train_loss= 1020776448.00000
Epoch: 0220 train_loss= 1020777152.00000
Epoch: 0230 train_loss= 1020776448.00000
Epoch: 0240 train_loss= 1020778432.00000
Epoch: 0250 train_loss= 1020776576.00000
Epoch: 0260 train_loss= 1020776576.00000
Epoch: 0270 train_loss= 1020778048.00000
Epoch: 0280 train_loss= 1020775424.00000
Epoch: 0290 train_loss= 1020774976.00000
Epoch: 0300 train_loss= 1020907904.00000
0.6866351271437019


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 5.41481
Epoch: 0020 train_loss= 5.41477
Epoch: 0030 train_loss= 5.41478
Epoch: 0040 train_loss= 5.41474
Epoch: 0050 train_loss= 5.41473
Epoch: 0060 train_loss= 5.41471
Epoch: 0070 train_loss= 5.41467
Epoch: 0080 train_loss= 5.41479
Epoch: 0090 train_loss= 5.41471
Epoch: 0100 train_loss= 5.41467
0.7498455647131128
Epoch: 0110 train_loss= 5.41462
Epoch: 0120 train_loss= 5.41458
Epoch: 0130 train_loss= 5.41527
Epoch: 0140 train_loss= 5.41469
Epoch: 0150 train_loss= 5.41455
Epoch: 0160 train_loss= 5.41451
Epoch: 0170 train_loss= 5.41440
Epoch: 0180 train_loss= 5.41434
Epoch: 0190 train_losTraceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
s= 5.41428
Epoch: 0200 train_loss= 5.41423
0.7492738389775754
Epoch: 0210 train_loss= 5.41414
Epoch: 0220 train_loss= 5.41427
Epoch: 0230 train_loss= 5.41432
Epoch: 0240 train_loss= 5.41421
Epoch: 0250 train_loss= 5.41404
Epoch: 0260 train_loss= 5.41378
Epoch: 0270 train_loss= 5.41362
Epoch: 0280 train_loss= 5.41342
Epoch: 0290 train_loss= 5.41322
Epoch: 0300 train_loss= 5.41302
0.7482881321209639


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 5.61885
Epoch: 0020 train_loss= 5.37482
Epoch: 0030 train_loss= 5.28285
Epoch: 0040 train_loss= 5.27965
Epoch: 0050 train_loss= 5.27540
Epoch: 0060 train_loss= 5.27538
Epoch: 0070 train_loss= 5.27473
Epoch: 0080 train_loss= 5.27470
Epoch: 0090 train_loss= 5.27465
Epoch: 0100 train_loss= 5.27463
0.4731843352741936
Epoch: 0110 train_loss= 5.27463
Epoch: 0120 train_loss= 5.27463
Epoch: 0130 train_loss= 5.27463
Epoch: 0140 train_loss= 5.27463
Epoch: 0150 train_loss= 5.27462
Epoch: 0160 train_loss= 5.27462
Epoch: 0170 train_loss= 5.27462
Epoch: 0180 train_loss= 5.27462
Epoch: 0190 train_loss= 5.27462
Epoch: 0200 train_loss= 5.27462
0.4732114834650256
Epoch: 0210 train_loss= 5.27462
Epoch: 0220 train_loss= 5.27462
Epoch: 0230 train_loss= 5.27462
Epoch: 0240 train_loss= 5.27462
Epoch: 0250 train_loss= 5.27462
Epoch: 0260 train_loss= 5.27462
Epoch: 0270 train_loss= 5.27462
Epoch: 0280 train_loss= 5.27462
Epoch: 0290 train_loss= 5.27462
Epoch: 0300 train_loss= 5.27462
0.4732015971296359


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 10.68227
Epoch: 0020 train_loss= 10.68055
Epoch: 0030 train_loss= 10.68017
Epoch: 0040 train_loss= 10.68028
Epoch: 0050 train_loss= 10.67995
Epoch: 0060 train_loss= 10.67958
Epoch: 0070 train_loss= 10.68299
Epoch: 0080 train_loss= 10.68022
Epoch: 0090 train_loss= 10.67923
Epoch: 0100 train_loss= 10.67888

The following have been reloaded with a version change:
  1) GCC/4.9.2 => GCC/6.2.0
  2) Python/2.7.9-intel-2016.u3 => Python/2.7.13-intel-2017.u2
  3) SQLite/3.8.11.1-intel-2016.u3 => SQLite/3.13.0-intel-2017.u2
  4) Tcl/8.6.5-intel-2016.u3 => Tcl/8.6.5-intel-2017.u2
  5) Tk/8.6.5-intel-2016.u3 => Tk/8.6.5-intel-2017.u2
  6) bzip2/1.0.6-intel-2016.u3 => bzip2/1.0.6-intel-2017.u2
  7) icc/2016.u3-GCC-4.9.2 => icc/2017.u2-GCC-6.2.0
  8) iccifort/2016.u3-GCC-4.9.2 => iccifort/2017.u2-GCC-6.2.0
  9) ifort/2016.u3-GCC-4.9.2 => ifort/2017.u2-GCC-6.2.0
 10) iimpi/2016.u3-GCC-4.9.2 => iimpi/2017.u2-GCC-6.2.0
 11) imkl/11.3.3.210-iimpi-2016.u3-GCC-4.9.2 => imkl/2017.2.174-iimpi-2017.u2-GCC-6.2.0
 12) impi/5.1.3.223-iccifort-2016.u3-GCC-4.9.2 => impi/2017.2.174-iccifort-2017.u2-GCC-6.2.0
 13) intel/2016.u3 => intel/2017.u2
 14) libffi/3.2.1-intel-2016.u3 => libffi/3.2.1-intel-2017.u2
 15) libreadline/6.3-intel-2016.u3 => libreadline/6.3-intel-2017.u2
 16) ncurses/5.9-intel-2016.u3 => ncurses/6.0-intel-2017.u2
 17) zlib/1.2.8-intel-2016.u3 => zlib/1.2.8-intel-2017.u2

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: spartan-gpgpu041.hpc.unimelb.edu.au
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.51.05  Sun Jun 28 10:33:40 UTC 2020
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 450.51.5
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 450.51.5
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 1.15778
Epoch: 0020 train_loss= 1.10306
Epoch: 0030 train_loss= 1.09624
Epoch: 0040 train_loss= 1.08896
Epoch: 0050 train_loss= 1.08748
Epoch: 0060 train_loss= 1.08740
Epoch: 0070 train_loss= 1.08718
Epoch: 0080 train_loss= 1.08701
Epoch: 0090 train_loss= 1.08691
Epoch: 0100 train_loss= 1.08682
0.7924574002813525
Epoch: 0110 train_loss= 1.08673
Epoch: 0120 train_loss= 1.08664
Epoch: 0130 train_loss= 1.08655
Epoch: 0140 train_loss= 1.08644
Epoch: 0150 train_loss= 1.08634
Epoch: 0160 train_loss= 1.08622
Epoch: 0170 train_loss= 1.08610
Epoch: 0180 train_loss= 1.08598
Epoch: 0190 train_loss= 1.08584
Epoch: 0200 train_loss= 1.08570
0.7921833953204206
Epoch: 0210 train_loss= 1.08556
Epoch: 0220 train_loss= 1.08541
Epoch: 0230 train_loss= 1.08526
Epoch: 0240 train_loss= 1.08512
Epoch: 0250 train_loss= 1.08499
Epoch: 0260 train_loss= 1.08486
Epoch: 0270 train_loss= 1.08474
Epoch: 0280 train_loss= 1.08462
Epoch: 0290 train_loss= 1.08450
Epoch: 0300 train_loss= 1.08444
0.7950897637304591


---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 21
Epoch: 0010 train_loss= 185008.20312
Epoch: 0020 train_loss= 184750.51562
Epoch: 0030 train_loss= 184717.46875
Epoch: 0040 train_loss= 184725.17188
Epoch: 0050 train_loss= 184722.45312
Epoch: 0060 train_loss= 184719.76562
Epoch: 0070 train_loss= 184719.20312
Epoch: 0080 train_loss= 184719.06250
Epoch: 0090 train_loss= 184717.50000
Epoch: 0100 train_loss= 184718.31250
0.6252312435765673
Epoch: 0110 train_loss= 184717.31250
Epoch: 0120 train_loss= 184717.20312
Epoch: 0130 train_loss= 184717.42188
Epoch: 0140 train_loss= 184717.32812
Epoch: 0150 train_loss= 184717.29688
Epoch: 0160 train_loss= 184719.29688
Epoch: 0170 train_loss= 184717.50000
Epoch: 0180 train_loss= 184717.46875
Epoch: 0190 train_loss= 184717.34375
Epoch: 0200 train_loss= 184719.45312
0.6252697841726619
Epoch: 0210 train_loss= 184719.15625
Epoch: 0220 train_loss= 184717.45312
Epoch: 0230 train_loss= 184717.37500
Epoch: 0240 train_loss= 184719.12500
Epoch: 0250 train_loss= 184718.34375
Epoch: 0260 train_loss= 184717.18750
Epoch: 0270 train_loss= 184717.46875
Epoch: 0280 train_loss= 184717.54688
Epoch: 0290 train_loss= 184718.39062
Epoch: 0300 train_loss= 184720.07812
0.6247687564234327


---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 8189
Epoch: 0010 train_loss= 1.17349
Epoch: 0020 train_loss= 1.17306
Epoch: 0030 train_loss= 1.17283
Epoch: 0040 train_loss= 1.17262
Epoch: 0050 train_loss= 1.17247
Epoch: 0060 train_loss= 1.17231
Epoch: 0070 train_loss= 1.17223
Epoch: 0080 train_loss= 1.17216
Epoch: 0090 train_loss= 1.17209
Epoch: 0100 train_loss= 1.17203
0.7791887388634177
Epoch: 0110 train_loss= 1.17197
Epoch: 0120 train_loss= 1.17192
Epoch: 0130 train_loss= 1.17188
Epoch: 0140 train_loss= 1.17191
Epoch: 0150 train_loss= 1.17180
Epoch: 0160 train_loss= 1.17174
Epoch: 0170 train_loss= 1.17169
Epoch: 0180 train_loss= 1.17176
Epoch: 0190 train_loss= 1.17157
Epoch: 0200 train_loss= 1.17147
0.7790708986821083
Epoch: 0210 train_loss= 1.17150
Epoch: 0220 train_loss= 1.17148
Epoch: 0230 train_loss= 1.17131
Epoch: 0240 train_loss= 1.17129
Epoch: 0250 train_loss= 1.17122
Epoch: 0260 train_loss= 1.17112
Epoch: 0270 train_loss= 1.17106
Epoch: 0280 train_loss= 1.17103
Epoch: 0290 train_loss= 1.17104
Epoch: 0300 train_loss= 1.17101
0.7789996464794562


---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 28
Epoch: 0010 train_loss= 11417.50293
Epoch: 0020 train_loss= 11405.17773
Epoch: 0030 train_loss= 11391.19531
Epoch: 0040 train_loss= 11320.68164
Epoch: 0050 train_loss= 11200.99512
Epoch: 0060 train_loss= 11353.08691
Epoch: 0070 train_loss= 11100.52832
Epoch: 0080 train_loss= 10947.45605
Epoch: 0090 train_loss= 10924.35645
Epoch: 0100 train_loss= 10886.93945
0.5677966101694916
Epoch: 0110 train_loss= 10835.62891
Epoch: 0120 train_loss= 10782.36719
Epoch: 0130 train_loss= 10804.86426
Epoch: 0140 train_loss= 10751.40527
Epoch: 0150 train_loss= 10720.09961
Epoch: 0160 train_loss= 10630.21387
Epoch: 0170 train_loss= 10710.66992
Epoch: 0180 train_loss= 10711.42480
Epoch: 0190 train_loss= 10584.25000
Epoch: 0200 train_loss= 10554.14160
0.536723163841808
Epoch: 0210 train_loss= 10684.83496
Epoch: 0220 train_loss= 10597.49414
Epoch: 0230 train_loss= 10498.15137
Epoch: 0240 train_loss= 10485.88086
Epoch: 0250 train_loss= 10510.98340
Epoch: 0260 train_loss= 10619.03027
Epoch: 0270 train_loss= 10540.21387
Epoch: 0280 train_loss= 10623.77637
Epoch: 0290 train_loss= 10485.42090
Epoch: 0300 train_loss= 10474.29395
0.5070621468926554


---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 18
Epoch: 0010 train_loss= 1134319232.00000
Epoch: 0020 train_loss= 1134242432.00000
Epoch: 0030 train_loss= 1134205952.00000
Epoch: 0040 train_loss= 1134206592.00000
Epoch: 0050 train_loss= 1134209280.00000
Epoch: 0060 train_loss= 1134207488.00000
Epoch: 0070 train_loss= 1134206720.00000
Epoch: 0080 train_loss= 1134206720.00000
Epoch: 0090 train_loss= 1134206720.00000
Epoch: 0100 train_loss= 1134206848.00000
0.6904494382022472
Epoch: 0110 train_loss= 1134206208.00000
Epoch: 0120 train_loss= 1134209664.00000
Epoch: 0130 train_loss= 1134208000.00000
Epoch: 0140 train_loss= 1134239104.00000
Epoch: 0150 train_loss= 1134207232.00000
Epoch: 0160 train_loss= 1134206208.00000
Epoch: 0170 train_loss= 1134206848.00000
Epoch: 0180 train_loss= 1134206208.00000
Epoch: 0190 train_loss= 1134207744.00000
Epoch: 0200 train_loss= 1134206720.00000
0.6900798344175044
Epoch: 0210 train_loss= 1134204800.00000
Epoch: 0220 train_loss= 1134204416.00000
Epoch: 0230 train_loss= 1134201472.00000
Epoch: 0240 train_loss= 1134226560.00000
Epoch: 0250 train_loss= 1134202752.00000
Epoch: 0260 train_loss= 1134201856.00000
Epoch: 0270 train_loss= 1134201472.00000
Epoch: 0280 train_loss= 1134301184.00000
Epoch: 0290 train_loss= 1134246912.00000
Epoch: 0300 train_loss= 1134233984.00000
0.6885866351271438


---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 1.18123
Epoch: 0020 train_loss= 1.18119
Epoch: 0030 train_loss= 1.18116
Epoch: 0040 train_loss= 1.18111
Epoch: 0050 train_loss= 1.18104
Epoch: 0060 train_loss= 1.18100
Epoch: 0070 train_loss= 1.18092
Epoch: 0080 train_loss= 1.18085
Epoch: 0090 train_loss= 1.18079
Epoch: 0100 train_loss= 1.18073
0.7483703925492853
Epoch: 0110 train_loss= 1.18066
Epoch: 0120 train_loss= 1.18059
Epoch: 0130 train_loss= 1.18056
Epoch: 0140 train_loss= 1.18040
Epoch: 0150 train_loss= 1.18026
Epoch: 0160 train_loss= 1.18008
Epoch: 0170 train_loss= 1.17997
Epoch: 0180 train_loss= 1.1Traceback (most recent call last):
  File "run.py", line 60, in <module>
    runner.erun()
  File "/data/gpfs/projects/punim1343/repo/GCN_AnomalyDetection/gae/anomaly_detection.py", line 55, in erun
    auc = roc_auc_score(y_true, reconstruction_errors)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/ranking.py", line 260, in roc_auc_score
    sample_weight=sample_weight)
  File "/usr/local/easybuild/software/scikit-learn/0.18-intel-2016.u3-Python-2.7.9/lib/python2.7/site-packages/sklearn/metrics/base.py", line 81, in _average_binary_score
    raise ValueError("{0} format is not supported".format(y_type))
ValueError: multiclass format is not supported
7975
Epoch: 0190 train_loss= 1.17966
Epoch: 0200 train_loss= 1.17942
0.7465895015522321
Epoch: 0210 train_loss= 1.17916
Epoch: 0220 train_loss= 1.17914
Epoch: 0230 train_loss= 1.17901
Epoch: 0240 train_loss= 1.17856
Epoch: 0250 train_loss= 1.17815
Epoch: 0260 train_loss= 1.17810
Epoch: 0270 train_loss= 1.17762
Epoch: 0280 train_loss= 1.17758
Epoch: 0290 train_loss= 1.17700
Epoch: 0300 train_loss= 1.17677
0.7448483224860929


---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 20
Epoch: 0010 train_loss= 1.45684
Epoch: 0020 train_loss= 1.44532
Epoch: 0030 train_loss= 1.44468
Epoch: 0040 train_loss= 1.44385
Epoch: 0050 train_loss= 1.44387
Epoch: 0060 train_loss= 1.44381
Epoch: 0070 train_loss= 1.44379
Epoch: 0080 train_loss= 1.44378
Epoch: 0090 train_loss= 1.44378
Epoch: 0100 train_loss= 1.44378
0.5101425954800303
Epoch: 0110 train_loss= 1.44378
Epoch: 0120 train_loss= 1.44378
Epoch: 0130 train_loss= 1.44378
Epoch: 0140 train_loss= 1.44378
Epoch: 0150 train_loss= 1.44378
Epoch: 0160 train_loss= 1.44378
Epoch: 0170 train_loss= 1.44378
Epoch: 0180 train_loss= 1.44378
Epoch: 0190 train_loss= 1.44378
Epoch: 0200 train_loss= 1.44378
0.5101870055263045
Epoch: 0210 train_loss= 1.44378
Epoch: 0220 train_loss= 1.44378
Epoch: 0230 train_loss= 1.44378
Epoch: 0240 train_loss= 1.44378
Epoch: 0250 train_loss= 1.44378
Epoch: 0260 train_loss= 1.44378
Epoch: 0270 train_loss= 1.44378
Epoch: 0280 train_loss= 1.44378
Epoch: 0290 train_loss= 1.44378
Epoch: 0300 train_loss= 1.44378
0.5102015996404512


---------------------------------------- Flickr3 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
feature number: 12047
Epoch: 0010 train_loss= 7.03259
Epoch: 0020 train_loss= 7.03240
Epoch: 0030 train_loss= 7.03235
Epoch: 0040 train_loss= 7.03231
Epoch: 0050 train_loss= 7.03221
Epoch: 0060 train_loss= 7.03213
Epoch: 0070 train_loss= 7.03215
Epoch: 0080 train_loss= 7.03121
Epoch: 0090 train_loss= 7.03053
Epoch: 0100 train_loss= 7.03004
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.5
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.43621531346351494
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7315436241610738
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.5042372881355932
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.5
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7303370786516854
---------------------------------------- Flickr2 ---------Traceback (most recent call last):
-------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7315436241610738
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.8637045849672227
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.6244861253854059
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.8519194247206776
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.5451977401129944
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.7310762862211708
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.1, 'weight_decay': 0.0, 'dropout': 0.0}
0.8953915867599688
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.8777598920738354
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.436292394655704
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.8135535391791199
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.5014124293785311
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.6903903015966883
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.2, 'weight_decay': 0.0, 'dropout': 0.0}
0.5572679316154335
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.844252706470054
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.6267343268242549
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.7994990422059682
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.5141242937853108
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.6903903015966883
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.7654780402477268
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.3, 'weight_decay': 0.0, 'dropout': 0.0}
0.5217472701160248
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.8465865381983758
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.6274280575539569
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.792297773916761
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.5338983050847458
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.689340626848019
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.4, 'weight_decay': 0.0, 'dropout': 0.0}
0.4921502497005853
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.851520048573511
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.6254110996916753
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.787971943074971
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.5268361581920904
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.6938941454760497
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.7550892730510423
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.5, 'weight_decay': 0.0, 'dropout': 0.0}
0.5048552891580483
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.8792882355343696
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.6251798561151078
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.78521366069153
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.5296610169491526
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.6900946185688942
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.7540257182028776
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.6, 'weight_decay': 0.0, 'dropout': 0.0}
0.4861782754213776
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.7754022493880942
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.6260534429599178
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.782909611099997
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.5437853107344632
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.6881283264340627
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.7480104637786218
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.7, 'weight_decay': 0.0, 'dropout': 0.0}
0.4912396083630237
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.8581738532529221
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.6264131551901336
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.7815345806122758
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.4731638418079096
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.6902424600827912
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.7491838567849094
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.8, 'weight_decay': 0.0, 'dropout': 0.0}
0.48664293318469304
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.7830201767863586
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.6248586844809867
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.7800896681565684
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.5409604519774012
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.6866351271437019
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.7482881321209639
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 0.9, 'weight_decay': 0.0, 'dropout': 0.0}
0.4732015971296359
---------------------------------------- acm_test_final ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7950897637304591
---------------------------------------- Amazon ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.6247687564234327
---------------------------------------- BlogCatalog ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7789996464794562
---------------------------------------- Disney ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.5070621468926554
---------------------------------------- Enron ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.6885866351271438
---------------------------------------- Flickr1 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.7448483224860929
---------------------------------------- Flickr2 ----------------------------------------
{'hidden3': 16, 'hidden2': 32, 'hidden1': 64, 'features': 1, 'learning_rate': 0.005, 'discriminator_out': 0, 'discriminator_learning_rate': 0.005, 'iterations': 300, 'alpha': 1.0, 'weight_decay': 0.0, 'dropout': 0.0}
0.5102015996404512
